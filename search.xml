<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>cs224w-06</title>
      <link href="posts/1431756251/"/>
      <url>posts/1431756251/</url>
      
        <content type="html"><![CDATA[<h3 id="1-A-Single-Layer-of-a-GNN"><a href="#1-A-Single-Layer-of-a-GNN" class="headerlink" title="1. A Single Layer of a GNN"></a>1. A Single Layer of a GNN</h3><p>GNN Layer = Message + Aggregation</p><h4 id="1-1-Message-Computation"><a href="#1-1-Message-Computation" class="headerlink" title="1.1 Message Computation"></a>1.1 Message Computation</h4><script type="math/tex; mode=display">m_u^{(l)}=MSG^{(l)}(h_u^{(l-1)})</script><ul><li><strong>Intuition: </strong>Each node will create a message, which will be sent to other nodes later</li><li><strong>Example: </strong>$m_u^{(l)}=W^{(l)}h_u^{(l-1)}$</li></ul><h4 id="1-2-Message-Aggregation"><a href="#1-2-Message-Aggregation" class="headerlink" title="1.2 Message Aggregation"></a>1.2 Message Aggregation</h4><script type="math/tex; mode=display">h_v^{(l)}=AGG^{(l)}\left(\left\{m_u^{(l)},u \in N(v)\right\}\right)</script><ul><li><p><strong>Intuition: </strong>Each node will aggregate the messages from node $v$’s neighbors</p></li><li><p><strong>Example: </strong>Sum($\cdot$), Mean($\cdot$) or Max($\cdot$)</p></li></ul><h4 id="1-3-Issue"><a href="#1-3-Issue" class="headerlink" title="1.3 Issue"></a>1.3 Issue</h4><p>来自结点 $v$ 本身的信息可能会丢失，$h_v^{(l)}$ 并不直接依赖 $h_v^{(l-1)}$</p><p>解决方法是计算 $h_v^{(l)}$ 时包含 $h_v^{(l-1)}$</p><ul><li><strong>Message: </strong><ul><li>$m_u^{(l)}=W^{(l)}h_u^{(l-1)}$ </li><li>$m_v^{(l)}=B^{(l)}h_v^{(l-1)}$</li></ul></li><li><strong>Aggregation: </strong><ul><li>$h_v^{(l)}=CONCAT\left(AGG\left(\left\{m_u^{(l)},u \in N(v\right\}\right),m_v^{(l)}\right)$</li></ul></li></ul><h4 id="1-4-Putting-things-together"><a href="#1-4-Putting-things-together" class="headerlink" title="1.4 Putting things together"></a>1.4 Putting things together</h4><ul><li>Message: </li></ul><script type="math/tex; mode=display">m_u^{(l)}=MSG^{(l)}(h_u^{(l-1)}), u \in \left\{N(v)\cup v\right\}</script><ul><li>Aggregation: </li></ul><script type="math/tex; mode=display">h_v^{(l)}=AGG\left(\left\{m_u^{(l)},u \in N(v\right\},m_v^{(l)}\right)</script><ul><li>Nonlinearity activation: 增加表现力<ul><li>Often written as $\sigma(\cdot)$</li></ul></li></ul><hr><h3 id="2-Classical-GNN-Layers"><a href="#2-Classical-GNN-Layers" class="headerlink" title="2. Classical GNN Layers"></a>2. Classical GNN Layers</h3><h4 id="2-1-GCN"><a href="#2-1-GCN" class="headerlink" title="2.1 GCN"></a>2.1 GCN</h4><script type="math/tex; mode=display">h_v^{(l)}=\sigma\left(W^{(l)}\sum_{u \in N(v)}\frac{h_u^{(l-1)}}{|N(v)|}\right)</script><p>重写为</p><script type="math/tex; mode=display">h_v^{(l)}=\sigma\left(\sum_{u\in N(v)}W^{(l)}\frac{h_u^{(l-1)}}{|N(v)|}\right)</script><ul><li><strong>Message:</strong> $m_u^{(l)}=\frac{1}{|N(v)|}W^{(l)}h_u^{(l-1)}$</li><li><strong>Aggregation: </strong>$h_v^{(l)}=\sigma\left(Sum\left(\left\{m_u^{(l),},u\in N(v)\right\}\right)\right)$</li></ul><h4 id="2-2-GraphSAGE"><a href="#2-2-GraphSAGE" class="headerlink" title="2.2 GraphSAGE"></a>2.2 GraphSAGE</h4><script type="math/tex; mode=display">h_v^{(l)}=\sigma\left(W^{(l)}\cdot CONCAT\left(h_v^{(l-1)},AGG\left(\left\{h_u^{(l-1)},\forall u \in N(v)\right\}\right)\right)\right)</script><ul><li><p><strong>Message:</strong> AGG$(\cdot)$</p></li><li><p><strong>Two=stage Aggregation: </strong></p><ul><li><strong>Stage 1: </strong> Aggregate from node neighbors</li></ul><script type="math/tex; mode=display">h_{N(v)}^{(l)}\leftarrow AGG\left(\left\{h_u^{(l-1)},\forall u \in N(v)\right\}\right)</script><ul><li><strong>Stage 2: </strong> Further aggregate over the node itself</li></ul></li></ul><script type="math/tex; mode=display">h_v^{(l)}\leftarrow\sigma\left(W^{(l)}\cdot CONCAT\left(h_v^{(l-1)},h_{N(v)}^{(l)}\right)\right)</script><h5 id="2-2-1-GraphSAGE-Neighbor-Aggregation"><a href="#2-2-1-GraphSAGE-Neighbor-Aggregation" class="headerlink" title="2.2.1 GraphSAGE Neighbor Aggregation"></a>2.2.1 GraphSAGE Neighbor Aggregation</h5><ul><li><strong>Mean: </strong></li></ul><script type="math/tex; mode=display">AGG=\sum_{u\in N(v)}\frac{h_u^{(l-1)}}{|N(v)|}</script><ul><li><strong>Pool: </strong>Transform neighbor vectors and apply symmetric vector function Mean$(\cdot)$ or Max$(\cdot)$</li></ul><script type="math/tex; mode=display">AGG=Mean(\left\{MLP(h_u^{(l-1)}),\forall u \in N(v)\right\})</script><ul><li><strong>LSTM: </strong>Apply LSTM to reshuffled of neighbors</li></ul><script type="math/tex; mode=display">AGG=LSTM([h_u^{(l-1)},\forall u \in \pi(N(v))])</script><h5 id="2-2-2-GraphSAGE-L2-Normalization"><a href="#2-2-2-GraphSAGE-L2-Normalization" class="headerlink" title="2.2.2 GraphSAGE: L2 Normalization"></a>2.2.2 GraphSAGE: L2 Normalization</h5><ul><li><p><strong>Optional: Apply $\mathcal{l}_2$ normalization to $h_v^{(l)}$ at every layer</strong></p></li><li><script type="math/tex; mode=display">h_v^{(l)}\leftarrow\frac{h_v^{(l)}}{||h_v^{(l)}||_2},\forall v \in V</script><p>where$ \ ||u||_2=\sqrt{\sum_iu_i^2}$</p></li><li><p>没有 $l_2$ 正则化时，嵌入向量有着不同的尺度</p></li><li><p>在某些情况下，嵌入的正则化会导致性能改进</p></li><li><p>$l_2$ 正则化后，所有向量会有相同范数</p></li></ul><h4 id="2-3-GAT"><a href="#2-3-GAT" class="headerlink" title="2.3 GAT"></a>2.3 GAT</h4><script type="math/tex; mode=display">h_v^{(l)}=\sigma\left(\sum_{u\in N(v)}\alpha_{vu}W^{(l)}h_u^{(l-1)}\right)</script><p>$\alpha_{vu}$ 为注意力权重</p><h5 id="2-3-1-in-GCN-GraphSAGE"><a href="#2-3-1-in-GCN-GraphSAGE" class="headerlink" title="2.3.1 in GCN/GraphSAGE"></a>2.3.1 in GCN/GraphSAGE</h5><ul><li>$\alpha_{vu}=\frac{1}{|N(v)|}$ is the weighting factor of node $u$’s message to node $v$</li><li>All neighbors $u\in N(v)$ are equally important to node $v$</li><li>$\alpha_{vu}$ is defined explicitly based on the structural properties of the graph (node degree) </li></ul><h5 id="2-3-2-Attention"><a href="#2-3-2-Attention" class="headerlink" title="2.3.2 Attention"></a>2.3.2 Attention</h5><p><strong>Goal: </strong>得到图中每个结点到邻居的重要性</p><p><strong>Idea: </strong>遵循注意力策略来计算每个点的嵌入</p><ul><li>结点处理来自邻居的消息</li><li>隐含地为邻域中不同结点指定不同的权重</li></ul><p><strong>Attention Mechanism $a$</strong></p><ul><li><p>首先计算注意力系数 $e_{vu}$，基于结点 $u$ 和 $v$ 的消息</p><script type="math/tex; mode=display">e_{vu}=a(W^{(l)}h_u^{(l-1)},W^{(l)}h_v^{(l-1)})</script></li><li><p>归一化以得到最终注意力权重，因此 $\sum_{u \in N(v)}\alpha_{vu}=1$</p><script type="math/tex; mode=display">\alpha_{vu}=\frac{\exp(e_{vu})}{\sum_{k\in N(v)}\exp(e_{vu})}</script></li><li><p>现在可以根据注意力权重进行加权求和</p></li></ul><script type="math/tex; mode=display">h_v^{(l)}=\sigma\left(\sum_{u\in N(v)}\alpha_{vu}W^{(l)}h_u^{(l-1)}\right)</script><ul><li>那么<strong>Attention Mechanism $a$</strong> 应该怎么定义呢？</li></ul><p>有多种不同选择。例如：使用简单的线性层神经网络</p><script type="math/tex; mode=display">e_{AB}=a(W^{(l)}h_A^{(l-1)},W^{(l)}h_B^{(l-1)})\\=Linear(Concat(W^{(l)}h_A^{(l-1)},W^{(l)}h_B^{(l-1)}))</script><p>$a$ 的基本参数是联合训练的</p><ul><li>但这有时候很难学习，难以收敛。使用  Multi-head Attention</li><li>这是一种稳定的注意力机制的学习过程</li><li>有多个注意力分数</li></ul><script type="math/tex; mode=display">h_v^{(l)}[1]=\sigma\left(\sum_{u\in N(v)}\alpha_{vu}^1W^{(l)}h_u^{(l-1)}\right)\\h_v^{(l)}[2]=\sigma\left(\sum_{u\in N(v)}\alpha_{vu}^2W^{(l)}h_u^{(l-1)}\right)\\h_v^{(l)}[3]=\sigma\left(\sum_{u\in N(v)}\alpha_{vu}^3W^{(l)}h_u^{(l-1)}\right)</script><script type="math/tex; mode=display">h_v^{(l)}=AGG(h_v^{(l)}[1],h_v^{(l)}[2],h_v^{(l)}[3])</script><h5 id="2-2-3-Benefits-of-Attention-Mechanism"><a href="#2-2-3-Benefits-of-Attention-Mechanism" class="headerlink" title="2.2.3 Benefits of Attention Mechanism"></a>2.2.3 Benefits of Attention Mechanism</h5><p><strong>为不同邻居指定不同的重要性</strong></p><ul><li>Computationally efficient:<ul><li>注意力系数可以在所有的边上并行</li><li>在所有结点上，聚合可以并行化</li></ul></li><li>Storage efficient<ul><li>不超过 $O(V+E)$</li><li>固定数量的参数，与图大小无关</li></ul></li><li>Localized<ul><li>只关注局部网络邻居</li></ul></li><li>Inductive capability<ul><li>这是一个共享的边对边机制</li><li>不依赖全局图结构</li></ul></li></ul><h3 id="3-GNN-Layer-in-Practice"><a href="#3-GNN-Layer-in-Practice" class="headerlink" title="3. GNN Layer in Practice"></a>3. GNN Layer in Practice</h3><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-22_17-12-27.png" style="zoom:67%;"></p><ul><li>Batch Normalization: Stabilize nerual network training</li><li>Dropout: Prevent overfitting</li><li>Attention/Gating: Control the importance of a message</li><li>More: Any other useful deep learning modules</li></ul><h4 id="3-1-BN"><a href="#3-1-BN" class="headerlink" title="3.1 BN"></a>3.1 BN</h4><p><strong>Goal:</strong> 稳定神经网络训练</p><p><strong>Idea: </strong>给定一批输入</p><ul><li>将结点嵌入重新中心化为零均值</li><li>缩放以具有单位方差<ul><li>Input: $X\in\mathbb{R}^{N\times D}$, $N$ node embeddings</li><li>Training Parameters: $\gamma,\beta\in \mathbb{R}^D$</li><li>Output: $Y\in \mathbb{R}^{N\times D}$, Normalized node embeddings</li></ul></li></ul><script type="math/tex; mode=display">\begin{equation} \begin{split}\hat{X}_{i,j}&=\frac{X_{i,j}-\mu_j}{\sqrt{\sigma_j^2+\varepsilon}}\\Y_{i,j}&=\gamma_j\hat{X}_{i,j}+\beta_j \end{split}\end{equation}</script><h4 id="3-2-Dropout"><a href="#3-2-Dropout" class="headerlink" title="3.2 Dropout"></a>3.2 Dropout</h4><p><strong>Idea:</strong></p><ul><li>During training: with some probability $p$, randomly set neurons to zero</li><li>During testing: Use all the neurons for computation</li></ul><h4 id="3-3-Activation-Non-linearity"><a href="#3-3-Activation-Non-linearity" class="headerlink" title="3.3 Activation Non-linearity"></a>3.3 Activation Non-linearity</h4><ul><li><p>ReLU</p></li><li><p>Sigmoid</p></li><li><p>Parametric ReLU</p><p>$PReLU(x_i)=\max(x_i, 0)+a_i\min(x_i,0)$</p></li></ul><hr><h3 id="4-Stacking-Layers-of-a-GNN"><a href="#4-Stacking-Layers-of-a-GNN" class="headerlink" title="4. Stacking Layers of a GNN"></a>4. Stacking Layers of a GNN</h3><h4 id="4-1-How-to-construct-a-Graph-Neural-Network"><a href="#4-1-How-to-construct-a-Graph-Neural-Network" class="headerlink" title="4.1 How to construct a Graph Neural Network?"></a>4.1 How to construct a Graph Neural Network?</h4><ul><li><p><strong>The standard way: </strong>按顺序堆叠图神经网络</p></li><li><p><strong>Issue of stacking many GNN layers: </strong>GNN遭受 over-smoothing 问题的影响。</p></li><li>Over-Smoothing: 所有结点嵌入收敛到相似的值<ul><li>如果感受野太大了，那么基本上所有的神经网络都收集相同的信息，不同结点的最终输出也是相同的</li><li>但我们希望不同结点的嵌入不同</li></ul></li></ul><h4 id="4-2-Receptive-Field-of-a-GNN"><a href="#4-2-Receptive-Field-of-a-GNN" class="headerlink" title="4.2 Receptive Field of a GNN"></a>4.2 Receptive Field of a GNN</h4><p>Receptive field: the set of nodes that determine the embedding of a node of interest</p><ul><li><p>in a $K$-layer GNN, each node has a receptive field of $K$-hop neighborhood</p></li><li><p>一个结点的嵌入由它的感受野确定，如果两个结点有高度重叠的感受野，那么他们的嵌入也很可能高度相似。</p></li><li>堆叠许多 GNN 层 $\rightarrow$ 结点具有高度重叠的感受野 $\rightarrow$ 结点嵌入会非常相似 $\rightarrow$ 过度平滑问题</li></ul><h4 id="4-3-Lesson-1-Be-cautious-when-adding-GNN-layers"><a href="#4-3-Lesson-1-Be-cautious-when-adding-GNN-layers" class="headerlink" title="4.3 Lesson 1: Be cautious when adding GNN layers"></a>4.3 Lesson 1: Be cautious when adding GNN layers</h4><ul><li>Unlike neural networks in other domains, <strong>adding more GNN layers do not always help</strong></li><li><strong>Step 1: Analyze the necessary receptive field</strong> to slove problem<ul><li>by computing the diameter of the graph</li></ul></li><li><strong>Step 2: </strong>Set number of GNN layers $L$ to be a bit more than the receptive field. <strong>Do not set $L$ to  be unnecessarily large!</strong> </li><li>How to enhance the expressive power of a GNN, if the number of GNN layers is small?</li></ul><h4 id="4-4-Expressive-Power-for-Shallow-GNNs"><a href="#4-4-Expressive-Power-for-Shallow-GNNs" class="headerlink" title="4.4 Expressive Power for Shallow GNNs"></a>4.4 Expressive Power for Shallow GNNs</h4><ul><li><strong>Solution 1: </strong>Increase the expressive power within each GNN layer<ul><li>In our previous examples, each transformation or aggregation function only include one linear layer</li><li>We can make aggregation / transformation become a deep neural network</li></ul></li><li><strong>Solution 2: </strong>Add layers that do not pass messages<ul><li>A GNN does not necessarily only contain GNN layers</li><li>We can add <strong>MLP layers</strong> before and after GNN layers, as <strong>pre-process layers</strong> and <strong>post-process layers</strong><ul><li><strong>pre-process layers: </strong>Important when encoding node features is necessary. E.g., when nodes represent images/text</li><li><strong>post-process layers: </strong>important when reasoning / transformation over node embeddings are needed. E.g., graph classification, knowledge graphs</li></ul></li><li>In practice, adding these layers works great!</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-22_22-36-26.png" alt=""></p><h4 id="4-5-Lesson-2-Add-skip-connections-in-GNNs"><a href="#4-5-Lesson-2-Add-skip-connections-in-GNNs" class="headerlink" title="4.5 Lesson 2: Add skip connections in GNNs"></a>4.5 Lesson 2: Add skip connections in GNNs</h4><ul><li><strong>Observation from over-smoothing:</strong> Node embeddings in earlier GNN layers can sometimes better differentiate nodes</li><li><strong>Solution: </strong>We can increase the impact of earlier layers on the final node embeddings, <strong>by adding shortcuts in GNN</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-22_22-42-11.png" alt=""></p><h4 id="4-6-Why-do-skip-connections-work"><a href="#4-6-Why-do-skip-connections-work" class="headerlink" title="4.6 Why do skip connections work?"></a>4.6 Why do skip connections work?</h4><ul><li><strong>Intuition: </strong>Skip connections create a mixture of models</li><li>$N$ skip connections $\rightarrow$ $2^N$possible paths</li><li>Each path could have up to $N$ modules</li><li>We automatically  get <strong>a mixture of shallow GNNs and deep GNNs</strong></li></ul><h4 id="4-7-GCN-with-Skip-Connections"><a href="#4-7-GCN-with-Skip-Connections" class="headerlink" title="4.7 GCN with Skip Connections"></a>4.7 GCN with Skip Connections</h4><ul><li><p><strong>A standard GCN layer:</strong></p></li><li><script type="math/tex; mode=display">h_v^{(l)}=\sigma\left(\underbrace{\sum_{u\in N(v)}W^{(l)}\frac{h_u^{(l-1)}}{|N(v)|}}_{This\ is\ our\ F(x)}\right)</script></li><li><p><strong>A GCN layer with skip connection: </strong></p></li><li><script type="math/tex; mode=display">h_v^{(l)}=\sigma\left(\underbrace{\sum_{u\in N(v)}W^{(l)}\frac{h_u^{(l-1)}}{|N(v)|}}_{F(x)}+\underbrace{h_v^{(l-1)}}_x\right)</script></li></ul><h4 id="4-8-Other-options-of-Skip-Connections"><a href="#4-8-Other-options-of-Skip-Connections" class="headerlink" title="4.8 Other options of Skip Connections"></a>4.8 Other options of Skip Connections</h4><p>Directly skip to the last layer</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-22_22-55-51.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cs224w-05</title>
      <link href="posts/3428813921/"/>
      <url>posts/3428813921/</url>
      
        <content type="html"><![CDATA[<p><strong>Recap: Node Embeddings</strong></p><p><strong>Intuition: </strong>Map nodes to $d$-dimensional embeddings such that similar nodes in the graph are embedded close together</p><p><strong>Goal: </strong>$similarity(u,v)\approx z_v^Tz_u$</p><p><strong>Encoder and Decoder</strong></p><p>最简单的Encoder就是一个嵌入查找</p><blockquote><p><strong>Limitations:</strong></p><ul><li>$O(|V|)$ parameters are needed<ul><li>No sharing of parameters between nodes</li><li>Every node has its own unique embedding</li></ul></li><li>Inheretly “transductive”<ul><li>Cannot generate embeddings for nodes that are not seen during training</li></ul></li><li>Do not incorporate node features<ul><li>Many graph have features that we can and should leverage</li></ul></li></ul></blockquote><p><strong>Deep Graph Encoders</strong></p><p>$\mathrm{ENC}(v)=$ <strong>multiple layers of non-linear transformations based on graph structure</strong></p><p><em>注：所有的深度编码器也可以结合之前的结点相似度函数</em></p><p><strong>Tasks: </strong></p><ul><li>Node classification: Predict a type of a given node</li><li>Link prediction: Predict whether two nodes are linked</li><li>Community detection: Identify densely linked clusters of nodes</li><li>Network similarity: How similar are two (sub)networks</li></ul><h3 id="1-Basics-of-Deep-Learning"><a href="#1-Basics-of-Deep-Learning" class="headerlink" title="1. Basics of Deep Learning"></a>1. Basics of Deep Learning</h3><p>监督学习：给定输入 $x$, 目标是预测标签 $y$</p><p>公式化为一个优化问题：</p><script type="math/tex; mode=display">\min_{\Theta} \mathcal{L}(\boldsymbol{y},f(\boldsymbol{x}))</script><p>$\Theta$ 为优化的参数，可能包含一个或多个标量、向量、矩阵</p><p>$\mathcal{L}$ 损失函数</p><p>梯度向量：$\triangledown_{\Theta}\mathcal{L}=(\frac{\partial\mathcal{L}}{\partial\Theta_1},\frac{\partial\mathcal{L}}{\partial\Theta_2},\dots)$</p><p>迭代算法：$\Theta\leftarrow\Theta-\eta \frac{\partial\mathcal{L}}{\partial\Theta}$</p><p>SGD &amp; Minibatch SGD</p><p>Back-propagation</p><p>Non-linearity</p><p>MLP</p><h3 id="2-Deep-Learning-for-Graphs"><a href="#2-Deep-Learning-for-Graphs" class="headerlink" title="2. Deep Learning for Graphs"></a>2. Deep Learning for Graphs</h3><blockquote><p><strong>Assume we have a graph $G$</strong></p><p>$V$ is the vertex set</p><p>$A$ is the adjacency matrix (assume binary)</p><p>$X\in \mathbb{R}^{m\times |V|}$ is a matrix of node features</p><p>$v$ a node in $V$; $N(v)$ the set of neighbors of $v$</p></blockquote><h4 id="2-1-A-Naive-Approach"><a href="#2-1-A-Naive-Approach" class="headerlink" title="2.1 A Naive Approach"></a>2.1 A Naive Approach</h4><p>Join adjacency matrix and features and feed them into a deep nerual net</p><p>有几个问题：</p><ul><li>$O(|V|)$ 参数个数</li><li>这个模型不适用于不同尺寸的图形</li><li>这种方法对结点排序敏感，而图中没有固定的排序</li></ul><h4 id="2-2-Convolutional-Networks"><a href="#2-2-Convolutional-Networks" class="headerlink" title="2.2 Convolutional Networks"></a>2.2 Convolutional Networks</h4><p>考虑图像上的卷积，但对于图而言，并不清楚如何定义窗口的概念，也不清楚如何定义在图形上滑动窗口的概念。</p><p><strong>Idea: </strong></p><ul><li>transform information at the neighbors and combine it</li><li>node’s neighborhood defines a computation graph</li><li>learn how to propagate information across the graph to compute node features</li></ul><h5 id="2-2-1-Aggregate-Neighbors"><a href="#2-2-1-Aggregate-Neighbors" class="headerlink" title="2.2.1 Aggregate Neighbors"></a>2.2.1 Aggregate Neighbors</h5><p><strong>Key idea: </strong>基于目标结点周围邻域的局部结构生成结点嵌入</p><p><strong>Intuition</strong>: 与经典神经网络的根本不同的是每个结点都可以定义自己的神经网络架构或者每个结点都可以基于它周围的网络结构定义它自己的计算图。</p><p>现在我们要同时训练或学习多个架构？</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-17_22-36-40.png" alt=""></p><p>Deep Model: Many Layers</p><ul><li>模型可以是任意深度的</li><li>结点在每一层都有嵌入</li><li>结点 $u$ 在第0层的嵌入被初始化为输入特征 $x_u$</li><li>第 $k$ 层嵌入的信息来自距离 $k$ 跳远的结点</li></ul><p>此外，我们还需要置换不变的聚合运算符，我们需要聚合的结果与顺序无关。</p><p>那么盒子中放了什么？如何定义这些转换？怎么参数化？如何学习？</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-17_22-52-39.png" alt=""></p><h5 id="2-2-2-Basic-approach"><a href="#2-2-2-Basic-approach" class="headerlink" title="2.2.2 Basic approach"></a>2.2.2 Basic approach</h5><p>简单平均来自邻居的信息然后应用神经网络</p><script type="math/tex; mode=display">\begin{equation}\begin{aligned}&h_v^0=x_v\\&h_v^{(l+1)}=\sigma(W_l\sum_{u\in N(v)}\frac{h_u^{(l)}}{|N(v)|}+B_lh_v^{(l)}), \forall l\in \left\{0,\dots,L-1\right\}\\&z_v=h_v^{(L)}\end{aligned}\end{equation}</script><p>$h_v^0$ 是结点 $v$ 在第0层的嵌入为结点的特征表示</p><p>高阶嵌入：</p><ul><li>$h_v^{(l)}$ 是结点 $v$ 在第 $l$ 层的嵌入</li><li>用矩阵 $B_l$ 进行变换</li><li>$\sum_{u\in N(v)}\frac{h_u^{(l)}}{|N(v)|}$ 前一层邻居嵌入的平均</li><li>用矩阵 $W_l$ 进行变换</li><li>再通过一个非线性变换</li></ul><p>$z_v$ 是 $L$ 层邻居聚合后的嵌入</p><p>如果每个结点都有自己的计算架构，我们如何训练模型？</p><p>参数 $W_l$ 和 $B_l$ 不是由结点索引的，所有结点使用相同的变换矩阵</p><h5 id="2-2-3-矩阵形式"><a href="#2-2-3-矩阵形式" class="headerlink" title="2.2.3 矩阵形式"></a>2.2.3 矩阵形式</h5><p>$H^{(l)}=[h_1^{(l)}\dots h_{|V|}^{(l)}]^T$</p><p>$\sum_{u\in N_v}h_u^{(l)}=A_{v,:}H^{(l)}$</p><p>$D$ 为对角阵，$D_{v,v}=deg(v)=|N(v)|$，那么 $D_{v,v}^{-1}=1/|N(v)|$</p><script type="math/tex; mode=display">\sum_{u\in N(v)}\frac{h_u^{(l-1)}}{|N(v)|} \Longrightarrow H^{(l+1)}=D^{-1}AH^{(l)}</script><p>所以</p><script type="math/tex; mode=display">H^{(l+1)}=\sigma({\color{Red}\tilde{A}H^{(l)}W_l^T}+{\color {Blue}H^{(l)}B_l^T})</script><p>$\tilde{A}=D^{-1}A$ 是稀疏矩阵，在实际中，可以使用高效的稀疏矩阵乘法。</p><p>Red: neighborhood aggregation</p><p>Blue: self transformation</p><h5 id="2-2-4-Model-Design-Overview"><a href="#2-2-4-Model-Design-Overview" class="headerlink" title="2.2.4 Model Design: Overview"></a>2.2.4 Model Design: Overview</h5><ol><li>定义邻域聚合函数</li><li>定义嵌入的损失函数</li><li>在一批结点上训练</li><li>为需要的结点生成嵌入</li></ol><p>可以将我们的模型应用于训练期从未见过的结点，模型具有归纳能力。</p><h3 id="3-A-General-GNN-Framework"><a href="#3-A-General-GNN-Framework" class="headerlink" title="3. A General GNN Framework"></a>3. A General GNN Framework</h3><ul><li>GNN Layer = Message + Aggregation</li><li>Connect GNN layers into a GNN</li><li>Raw input graph $\neq$ computational graph<ul><li>Graph feature augmentation</li><li>Graph structure manipulation</li></ul></li><li>Learning objective<ul><li>Supervised/Unsupervised</li><li>Node/Edge/Graph level objectives</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-18_17-08-02.png" alt=""></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cs224w-04</title>
      <link href="posts/1203422807/"/>
      <url>posts/1203422807/</url>
      
        <content type="html"><![CDATA[<blockquote><p>给一个在某些结点上带有标签的网络，我们如何为网络中其他结点分配标签？</p></blockquote><h3 id="1-Message-Passing-and-Node-Classification"><a href="#1-Message-Passing-and-Node-Classification" class="headerlink" title="1. Message Passing and Node Classification"></a>1. Message Passing and Node Classification</h3><p>考虑结点分类问题：给定一些结点的标签，预测未标记结点的标签。这被叫做半监督结点分类问题。</p><h4 id="1-1-Framework：Message-Passing"><a href="#1-1-Framework：Message-Passing" class="headerlink" title="1.1 Framework：Message Passing"></a>1.1 Framework：<strong>Message Passing</strong></h4><p><strong>Intuition：</strong>网络中的相关性或相似的结点是相连的</p><p><strong>Key concept：</strong>collective classification 集体分类</p><p><strong>Techniques：</strong></p><ul><li>Rational classification</li><li>Iterative classification</li><li>Belief classification</li></ul><h4 id="1-2-Correlations-Exists-in-Networks"><a href="#1-2-Correlations-Exists-in-Networks" class="headerlink" title="1.2 Correlations Exists in Networks"></a>1.2 Correlations Exists in Networks</h4><p>网络中存在相关性是指个人行为在网络结构中是相关的。</p><p>相关性意味着附近的结点往往具有相同的颜色或属于同一类</p><ul><li><strong>Homophily</strong> </li></ul><p>The tendency of individuals to associate and bond with similar others</p><ul><li><strong>Influence</strong></li></ul><p>Social connections can influence the individual characteristics of a person</p><h4 id="1-3-Motivation"><a href="#1-3-Motivation" class="headerlink" title="1.3 Motivation"></a>1.3 Motivation</h4><ul><li>相似的结点通常靠近在一起或直接连接在网络中</li><li>结点 $v$ 的分类标签依赖于<ul><li>$v$ 的特征</li><li>结点 $v$ 邻居的标签</li><li>结点 $v$ 邻居的特征</li></ul></li></ul><h4 id="1-4-集体分类（Collective-Classification）的应用"><a href="#1-4-集体分类（Collective-Classification）的应用" class="headerlink" title="1.4 集体分类（Collective Classification）的应用"></a>1.4 集体分类（Collective Classification）的应用</h4><ul><li>文件分类</li><li>链接预测</li><li>光学字符识别</li><li>3D数据分类</li><li>传感器网络中的实体解析</li><li>垃圾邮件和欺诈检测</li><li>词性标注</li></ul><h4 id="1-5-集体分类综述"><a href="#1-5-集体分类综述" class="headerlink" title="1.5 集体分类综述"></a>1.5 集体分类综述</h4><p> Markov Assumption：the label $Y_v$ of one node $v$ depends on the labels of its neighbors $N_v$</p><script type="math/tex; mode=display">P(Y_v)=P(Y_v|N_v)</script><blockquote><p><strong>Three Steps</strong></p><ul><li>Local Classifier：Assign initial labels</li><li>Relational Classifier：Capture correlations between nodes</li><li>Collective Inference：Propagate correlations through network</li></ul></blockquote><h5 id="1-5-1-Local-Classifier：用于初始标签分配"><a href="#1-5-1-Local-Classifier：用于初始标签分配" class="headerlink" title="1.5.1 Local Classifier：用于初始标签分配"></a>1.5.1 Local Classifier：用于初始标签分配</h5><ul><li>根据结点的属性或特征预测结点标签</li><li>标准分类任务</li><li>没有使用网络信息</li></ul><h5 id="1-5-2-Relational-Classifier：捕获结点的相关性"><a href="#1-5-2-Relational-Classifier：捕获结点的相关性" class="headerlink" title="1.5.2 Relational Classifier：捕获结点的相关性"></a>1.5.2 Relational Classifier：捕获结点的相关性</h5><ul><li>学习一个分类器，基于邻域中结点的标签或属性预测某个结点的标签</li><li>使用网络信息的地方</li></ul><h5 id="1-5-3-Collective-Inference：传播相关性"><a href="#1-5-3-Collective-Inference：传播相关性" class="headerlink" title="1.5.3 Collective Inference：传播相关性"></a>1.5.3 Collective Inference：传播相关性</h5><ul><li>迭代地将关系分类器应用于每个结点</li><li>迭代到相邻节点之间的不一致最小化</li><li>网络结构将影响最终预测</li></ul><p><strong>Problem</strong></p><ul><li>给没被标签的 $v$ 预测标签 $Y_v$（灰色结点）</li><li>每个结点 $v$ 有特征向量 $f_v$</li><li>某些结点的标签给定（1是绿色，0是红色）</li><li>目标为所有特征和网络找到 $P(Y_v)$</li></ul><h3 id="2-Relational-Classification-and-Iterative-Classification"><a href="#2-Relational-Classification-and-Iterative-Classification" class="headerlink" title="2. Relational Classification and Iterative Classification"></a>2. Relational Classification and Iterative Classification</h3><h4 id="2-1-Probabilistic-Relational-Classifier"><a href="#2-1-Probabilistic-Relational-Classifier" class="headerlink" title="2.1 Probabilistic Relational Classifier"></a>2.1 Probabilistic Relational Classifier</h4><p><strong>Basic idea：</strong>结点 $v$ 的类概率 $Y_v$ 是其邻居的类概率的加权平均值</p><p>对于标记的结点，初始化 $Y_v$ 为真实 $Y_v^*$</p><p>对于未标记的结点，初始化 $Y_v=0.5$</p><p>更新结点知道收敛或达到最大迭代次数</p><p>对于每个结点 $v$ 和标签 $c$</p><script type="math/tex; mode=display">P(Y_v=c)=\frac{1}{\sum_{(v,u)\in E}A_{v,u}}\sum_{(v,u)\in E}A_{v,u}P(Y_u=c)</script><p><strong>Challenges：</strong></p><ul><li><p>公式收敛无法保证</p></li><li><p>模型无法使用结点特征信息</p></li></ul><h4 id="2-2-Iterative-Classification"><a href="#2-2-Iterative-Classification" class="headerlink" title="2.2 Iterative Classification"></a>2.2 Iterative Classification</h4><p><strong>Main idea：</strong>根据结点的属性 $f_v$ 和邻居节点 $N_v$ 的标签 $z_v$ 对结点 $v$ 分类</p><p><strong>Input：</strong></p><ul><li>$f_v$：每个结点都有对应的特征向量</li><li>某些结点被标记为 $Y_v$</li></ul><p><strong>Approach：</strong>训练两个分类器</p><ul><li><p>$\phi_1(f_v)$：基于结点特征 $f_v$ 用来预测结点的标签</p></li><li><p>$\phi_2(f_v,z_v)$：基于结点特征向量 $f_v$ 和邻居的标签汇总向量 $z_v$ 来预测标签</p></li></ul><p>如何计算标签汇总向量 $z_v$？可以选择以下方式</p><ul><li>$N_v$ 中每个标签数目（或比例）的直方图</li><li>$N_v$ 中最常见的标签</li><li>$N_v$ 中不同标签的数目</li></ul><h5 id="2-2-1-迭代分类器的架构"><a href="#2-2-1-迭代分类器的架构" class="headerlink" title="2.2.1 迭代分类器的架构"></a>2.2.1 迭代分类器的架构</h5><ul><li>Phase 1：仅根据结点属性对结点标签进行分类<ul><li>在训练集上，训练两个分类器</li><li>$\phi_1(f_v)$ 和 $\phi_2(f_v,z_v)$</li></ul></li><li>Phase 2：迭代直到收敛<ul><li>在测试集上，使用 $\phi_1$ 分配初始标签，计算 $z_v$ 然后使用 $\phi_2$ 预测标签</li><li>对每个结点重复<ul><li>基于 $Y_u$ 更新 $z_v$</li><li>基于 $z_v\ (\phi_2)$ 更新 $Y_v$</li></ul></li><li>迭代直到类标签稳定或达到最大迭代次数</li></ul></li><li>仍然不保证收敛</li></ul><h3 id="3-Loopy-Belief-Propagation"><a href="#3-Loopy-Belief-Propagation" class="headerlink" title="3. Loopy Belief Propagation"></a>3. Loopy Belief Propagation</h3><p>信念传播是一种动态规划方法用于回答图中的概率查询，即给定结点属于给点类的概率。这是一个迭代过程，相邻节点通过相互传递消息来交谈。</p><p><strong>Task：</strong>统计图中结点数</p><p><strong>Condition：</strong>每个结点只能与邻居传递消息</p><p>在点上定义一种排序，再根据排序定义边的方向，边的方向定义了消息传递的顺序，从 $i$ 到 $i+1$ 计算和传递消息</p><p>Each node listens to the message from its neighbor, updates it, and passes it forward.</p><ul><li><p>path graph</p></li><li><p>tree graph：消息从叶子传递到树的根结点</p></li></ul><h4 id="3-1-Loopy-BP-Algorithm"><a href="#3-1-Loopy-BP-Algorithm" class="headerlink" title="3.1 Loopy BP Algorithm"></a>3.1 Loopy BP Algorithm</h4><ul><li>Label-label potential matrix $\psi$：一个结点与邻居在标签方面的依赖，$\psi(Y_i,Y_j)$ 是，在给定邻居 $i$ 属于类 $Y_i$ 时，结点 $j$ 属于类 $Y_j$ 概率的比例</li><li>Prior belief $\phi$：$\phi(Y_i)$ 是结点 $i$ 在类 $Y_i$ 的概率比例</li><li>$m_{i\rightarrow j}(Y_j)$ 是结点 $i$ 的消息，关于结点 $j$ 在 $Y_j$类的估计</li><li>$\mathcal{L}$ 是所有类标签合集</li><li>先将所有消息初始化为1</li><li>对每个结点重复</li></ul><script type="math/tex; mode=display"> m_{i\rightarrow j}(Y_j)=\sum_{Y_i\in\mathcal{L}}\psi(Y_i,Y_j)\phi_i(Y_i)\prod_{k\in N_i\setminus j}m_{k\rightarrow i}(Y_i) ,\forall Y_j \in \mathcal{L}</script><p><strong>After convergence:</strong></p><p>$b_i(Y_i)=$ node $i$’s belief of being in class $Y_i$</p><script type="math/tex; mode=display">b_i(Y_i)=\phi_i(Y_i)\prod_{j\in N_i}m_{j\rightarrow i}(Y_j),\forall Y_i \in \mathcal{L}</script><hr><p>现在考虑含圈的图，结点不再有固定的排序。来自不同子图的信息不再独立，但仍然可以运行BP算法。</p><p><strong>Belief may not converge</strong></p><p>消息 $m_{u\rightarrow i}(Y_i)$ 是基于对 $i$ 的初始信念，而不是来自 $i$ 的独立证据，在循环中，某种信念可能会被循环不断放大。</p><p>但在实践中，Loopy BP 仍然是很好的启发式方法，循环的影响也会较小。</p><p><strong>Advantages</strong></p><ul><li>易于编码和并行化</li><li>适用性广泛</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cs224w-03</title>
      <link href="posts/624699732/"/>
      <url>posts/624699732/</url>
      
        <content type="html"><![CDATA[<blockquote><p> Graph as Matrix: PageRank, Random Walks and Embeddings</p></blockquote><p>Web as a directed graph.</p><p>所有的网页的重要性并不是相等的，网络图连通性存在如此大的多样性，那我们可以使用网络图链接结构来给页面排名吗？</p><p><strong>链接分析方法（Link Analysis approaches）</strong></p><ul><li>PageRank</li><li>Personalized PageRank (PPR)</li><li>Random Walk with Restarts</li></ul><hr><h3 id="1-PageRank"><a href="#1-PageRank" class="headerlink" title="1. PageRank"></a>1. PageRank</h3><h4 id="1-1-Links-as-votes-Page-is-more-important-if-it-has-more-links"><a href="#1-1-Links-as-votes-Page-is-more-important-if-it-has-more-links" class="headerlink" title="1.1 Links as votes: Page is more important if it has more links"></a>1.1 <strong>Links as votes:</strong> Page is more important if it has more links</h4><ul><li>Links from important pages count more</li><li>Recursive question</li></ul><blockquote><p><strong>Flow： </strong></p><p>if page $i$ with importance $r_i$ has $d_i$ out links, each link gets $r_i/d_i$ votes</p><p>$j$‘s own importance $r_j$ is the sum of the votes on its <strong>in-links</strong></p></blockquote><script type="math/tex; mode=display">r_j=\sum_{i\rightarrow j}\frac{r_i}{d_i}</script><p>$d_i$ is out-degree of node $i$</p><p><strong>随机邻接矩阵$M:$</strong></p><p>Let page $j$ have $d_j$ out-links, if $j \rightarrow i$, then $M_{ij}=\frac{1}{d_j}$, Columns sum to 1</p><p><strong>秩向量 $r:$</strong> An entry per page</p><p>$r_i$ 为页面 $i$ 的重要性，$\sum_ir_i=1$，可以视为一个概率分布。</p><p>那么流动方程可以写成：</p><script type="math/tex; mode=display">r=M\cdot r</script><h4 id="1-2-Connection-to-Random-Walk"><a href="#1-2-Connection-to-Random-Walk" class="headerlink" title="1.2 Connection to Random Walk"></a>1.2 Connection to Random Walk</h4><p>假设一个随机网页浏览者：</p><ul><li>在时间 $t$，浏览者在一些 $i$ 页面上</li><li>在时间 $t+1$，浏览者顺着 $i$ 的外链均匀随机访问导航到 $j$</li><li>过程无限重复</li></ul><p>令：</p><ul><li>$p(t)$ 是一个向量，第 $i$ 个坐标为时间 $t$ 浏览者在页面 $j$ 的概率</li><li>$p(t)$ 是页面上的概率分布</li></ul><p>同样：</p><script type="math/tex; mode=display">r_j=\sum_{i\rightarrow j}\frac{r_i}{d_{out}(i)}</script><script type="math/tex; mode=display">p(t+1)=M\cdot p(t)</script><p>那么如果：$p(t+1)=M \cdot p(t)=p(t)$，那么 $p(t)$ 为随机游走的平稳分布</p><p>即 $r=M \cdot r$ 中的 $r$ 为随机游走的平稳分布</p><p>考虑特征值：</p><script type="math/tex; mode=display">1\cdot r=M \cdot r</script><p>即秩向量 $r$ 是随机邻接矩阵 $M$ 特征值为1的特征向量。</p><blockquote><p><strong>PageRank = Limiting distribution = principal eigenvector of M</strong></p></blockquote><h4 id="1-3-How-to-solve"><a href="#1-3-How-to-solve" class="headerlink" title="1.3 How to solve?"></a>1.3 How to solve?</h4><h5 id="1-3-1-幂迭代"><a href="#1-3-1-幂迭代" class="headerlink" title="1.3.1 幂迭代"></a>1.3.1 <strong>幂迭代</strong></h5><ul><li>给每个结点分配初始秩</li><li>迭代到收敛 $\sum_i|r_i^{t+1}-r_i^t| &lt; \varepsilon$</li></ul><script type="math/tex; mode=display">r_j^{(t+1)}=\sum_{i \rightarrow j}\frac{r_i^{(t)}}{d_i}</script><h5 id="1-3-2-Power-iteration"><a href="#1-3-2-Power-iteration" class="headerlink" title="1.3.2 Power iteration"></a>1.3.2 <strong>Power iteration</strong></h5><ul><li>initialize: $r^0=[1/N,\dots,1/N]^T$</li><li>iterate: $r^{(t+1)}=M\cdot r^t$ <ul><li>$r_j^{(t+1)}=\sum_{i \rightarrow j}\frac{r_i^{(t)}}{d_i}$</li></ul></li><li>Stop when $|r^{(t+1)}-r^t|_1&lt;\varepsilon$</li></ul><h5 id="1-3-3-Two-problems"><a href="#1-3-3-Two-problems" class="headerlink" title="1.3.3 Two problems:"></a>1.3.3 <strong>Two problems:</strong></h5><ul><li><strong>Spider traps (all out-links are within the group)</strong></li></ul><p>Eventually spider traps absorb all importance</p><p>比如 $a$ 指向 $b$, $b$ 指向自己</p><ul><li><strong>Some pages are dead ends (have no out-links)</strong></li></ul><p>Such pages cause importance to leak out</p><p>比如 $a$ 指向 $b$, $b$ 没有外链</p><h5 id="1-3-4-Solution-to-Spider-Traps"><a href="#1-3-4-Solution-to-Spider-Traps" class="headerlink" title="1.3.4 Solution to Spider Traps"></a>1.3.4 Solution to Spider Traps</h5><ul><li>每个时间步，随机浏览者有两个选项：<ul><li>以 $\beta$ 的概率，随机访问一个链接</li><li>以 $1-\beta$ 的概率，跳转到随机页面</li><li>$\beta \in [0.8, 0.9]$</li></ul></li><li>浏览者会在几步之中传送出去</li></ul><h5 id="1-3-5-Solution-to-Dead-Ends"><a href="#1-3-5-Solution-to-Dead-Ends" class="headerlink" title="1.3.5 Solution to Dead Ends"></a>1.3.5 Solution to Dead Ends</h5><p>如果矩阵某列的和为0，则一定是Dead Ends，则修改矩阵，让他以相等概率选择任一链接。</p><h5 id="1-3-6-Random-Teleports"><a href="#1-3-6-Random-Teleports" class="headerlink" title="1.3.6 Random Teleports"></a>1.3.6 Random Teleports</h5><blockquote><p>Google’s solution:</p><p>With prob. $\beta$, follow a link at random</p><p>with prob. $1-\beta$, jump to some random page</p></blockquote><p><strong>PageRank Equation</strong></p><script type="math/tex; mode=display">r_j=\sum_{i\rightarrow j}\beta\frac{r_i}{d_i}+(1-\beta)\frac{1}{N}</script><p>这个公式假设 $M$ 没有 dead ends，可以预处理或使用上文方法进行处理。</p><p>写成矩阵形式：</p><script type="math/tex; mode=display">G=\beta M +(1-\beta)\left[\frac{1}{N}\right]_{N\times N},r=G\cdot r</script><p>$\beta\in[0.8,0.9]$</p><hr><h3 id="2-Random-Walk-with-Restarts-and-Personalized-PageRank"><a href="#2-Random-Walk-with-Restarts-and-Personalized-PageRank" class="headerlink" title="2. Random Walk with Restarts and Personalized PageRank"></a>2. Random Walk with Restarts and Personalized PageRank</h3><p><strong>Bipartite User-Item Graph</strong></p><p>What items should we recommend to a user who interacts with item Q?</p><p><strong>intuition: </strong>if items Q and P are interacted by similar users, recommend P when user interacts with Q.</p><p>怎么比较多种上述关系的强弱？</p><p><strong>PageRank:</strong></p><p>提供图上结点的重要性并对其进行排名，同时一个随机的冲浪者在图中的任何结点上均匀传送。</p><p><strong>Personalized PageRank:</strong></p><p>每当传送时，只能跳回结点子集 $S$ 。</p><p>如果 $S$ 是单个结点怎么办？</p><p><strong>Random Walk with Restarts:</strong></p><p>Teleport back to the starting node: $S=\left\{Q\right\}$</p><p>这样的相似性考虑了多种因素：</p><ul><li>Multiple connection</li><li>Mutilple paths</li><li>Direct and indirect connections</li><li>Degree of the node</li></ul><hr><h3 id="3-结点嵌入与矩阵分解"><a href="#3-结点嵌入与矩阵分解" class="headerlink" title="3. 结点嵌入与矩阵分解"></a>3. 结点嵌入与矩阵分解</h3><p>之前通过$z_v^Tz_u$考虑结点对 $(u,v)$ 的相似性</p><p>这里我们考虑最简单的结点相似度：$u,v$ 相似如果他们相连</p><p>那么 $\boldsymbol{z_v^Tz_u}=A_{u,v}$</p><p>即</p><script type="math/tex; mode=display">Z^TZ=A</script><p>同时，嵌入的维度 $d$ （$Z$ 的行数）显然比结点数 $n$ 要小很多</p><p>所以 $A= Z^TZ$ 的精确解往往难以找到</p><p>我们可以优化以下目标：</p><script type="math/tex; mode=display">\min_{Z}||A-Z^TZ||_2</script><ul><li>优化 $Z$ 以最小化L2范数</li><li>之前使用的是softmax函数而不是L2范数，但利用 $Z^TZ$ 来逼近 $A$ 的目标是相同的</li><li>结论：通过边连通性定义的，与结点相似度有关的内积解码器等价于 $A$ 的矩阵分解</li></ul><hr><p>DeepWalk 等价于以下复杂矩阵分解</p><script type="math/tex; mode=display">log (vol(G)(\frac{1}{T}\sum_{r=1}^{T}(D^{-1}A)^r)D^{-1})-\log b</script><p>$D$ 对角阵，$D_{u,u}=\deg(u)$</p><p>$r$ 是正则邻接矩阵的幂</p><p>$T$ 是上下文窗口长度，即随机游走的长度</p><p>$vol(G)$ $G$的体积，$vol(G)=\sum_i\sum_jA_{i,j}$</p><p>$\log b$ 对于使用的负样本数量</p><p><em>Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec, WSDM 18</em></p><p><strong>Limitation</strong></p><ul><li>无法为新加入图的结点完成嵌入</li><li>无法捕获结构相似性</li><li>无法整合结点、边和图特征</li></ul><p><strong>Solution:</strong></p><p><strong>Deep Representation Learning and Graph Neural Networks</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cs224w-02</title>
      <link href="posts/1379604930/"/>
      <url>posts/1379604930/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Node-Embeddings"><a href="#1-Node-Embeddings" class="headerlink" title="1. Node Embeddings"></a>1. Node Embeddings</h3><p>Graph Representation learning alleviates the need to do feature engineering every single time.</p><p><strong>Goal:</strong>使用图进行机器学习的高效任务独立特征学习</p><p>对于每个结点，我们想学习如何在$d$维中映射这个结点，将这个$d$个数字组成的向量称为特征表示或者称为嵌入（Feature representation, embedding）</p><p><strong>Task:</strong>将结点映射到嵌入空间中</p><ul><li>结点嵌入的相似性表示它们在网络中的相似性</li><li>编码网络信息</li><li>用于不同下游预测任务</li></ul><h4 id="1-1-Encoder-and-Decoder"><a href="#1-1-Encoder-and-Decoder" class="headerlink" title="1.1 Encoder and Decoder"></a>1.1 Encoder and Decoder</h4><ul><li>$V$ is the vertex set</li><li>$A$ is the adjacency matrix</li><li>For simplicity: no node features or extra information is used</li><li><strong>Goal</strong> is to encode nodes so that similarity in the embedding space (e.g. dot product) approximates similarity in the graph</li></ul><script type="math/tex; mode=display">similarity(u,v) \approx \boldsymbol{Z_v^TZ_u}</script><ul><li>编码器（Encoder）将结点映射到嵌入</li><li>定义一个结点相似度函数（原网络相似度的一种度量）</li><li>解码器（Decoder）将嵌入映射到相似度得分，甚至可以包含点积这种</li><li>优化编码器参数满足上面的Goal</li></ul><p>最简单的Encoding方式：Encoder只是一个嵌入查找</p><script type="math/tex; mode=display">ENC(v)=\boldsymbol{z_v}=\boldsymbol{Z}\cdot v</script><p>$\boldsymbol{Z}\in \mathbb{R}^{d \times |V|}$：矩阵的每一列都是一个嵌入，也是我们学习或优化的。</p><p>$v \in \mathbb{I}^{|V|}$：全0指示向量，除指示结点$v$的那列为1</p><p>但这样的参数实在太多，我们方法的目标是直接优化每个结点的嵌入。</p><ul><li><strong>DeepWalk</strong></li><li><strong>node2vec</strong></li></ul><p><em>注：随机游走是一种无监督学习结点嵌入的方式，不使用结点标签与特征，这里的目标是直接估计结点的一组坐标（嵌入），所以网格结构的某些方面（通过DEC捕捉）得以保留</em></p><h4 id="1-2-Random-Walk-Approaches-for-Node-Embeddings"><a href="#1-2-Random-Walk-Approaches-for-Node-Embeddings" class="headerlink" title="1.2 Random Walk Approaches for Node Embeddings"></a>1.2 Random Walk Approaches for Node Embeddings</h4><ul><li><strong>Goal</strong>: $u$的嵌入$z_u$</li><li><strong>Probability</strong>: 给定结点$u$的相似程度的先验概率，用随机游走来定义，从结点$u$开始的随机游走访问结点$v$的先验概率。</li><li><strong>Softmax function</strong> and <strong>Sigmoid function</strong></li><li>Random Walk</li></ul><script type="math/tex; mode=display">\boldsymbol{Z_u^TZ_v}\approx \mathrm{probability\ that}\ u\ \mathrm{and}\ v\  \mathrm{cooccur}\ on \\\ a \ random\ walk\ over\ the\ graph</script><ol><li>从结点$u$开始使用随机游走策略$R$，估计访问结点$v$概率$P_R(v|u)$</li><li>优化嵌入去编码随机游走 $\cos \theta \propto P_R(v|u)$</li></ol><blockquote><p><strong>Why Random Walks？</strong></p><ol><li><strong>Expressivity：</strong>提供了一个灵活的结点相似性随机定义，结合局部和高阶邻居的信息。<strong>如果从结点$u$开始以高概率访问$v$，则$u$和$v$很可能相似。</strong></li><li><strong>Efficiency：</strong>不需要在训练是考虑所有结点对，只需要考虑在随机游走中共同出现是对。</li></ol></blockquote><p>Given $G=(V,E)$</p><p><strong>Goal：</strong>学习一个映射$f:u\rightarrow \mathbb{R}^d:f(u)=\boldsymbol{z}_u$</p><p>对数似然函数：</p><script type="math/tex; mode=display">\max_f \sum_{u \in V}\log P(N_R(u)|\boldsymbol{z_u})</script><p>$N_R(u)$是结点$u$使用策略$R$得到的邻居</p><hr><h5 id="1-2-1-Random-Walk-Optimization"><a href="#1-2-1-Random-Walk-Optimization" class="headerlink" title="1.2.1 Random Walk Optimization"></a>1.2.1 Random Walk Optimization</h5><ol><li>从结点$u$运行短固定长度随机游走，使用的策略为$R$</li><li>收集$N_R(u)$，使用多重集收集随机游走访问的结点</li><li>优化嵌入：给定结点$u$，预测邻居$N_R(u)$</li></ol><script type="math/tex; mode=display">\max_f \sum_{u \in V}\log P(N_R(u)|\boldsymbol{z_u})</script><p>等价地</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{u \in V}\sum_{v\in N_R(u)}-\log(P(v|\boldsymbol{z_u}))</script><p>使用softmax参数化$P(v|\boldsymbol{z_u})$</p><script type="math/tex; mode=display">P(v|\boldsymbol{z_u})=\frac{\exp(\boldsymbol{z_u^Tz_n})}{\sum_{n\in V}\exp(\boldsymbol{z_u^Tz_n})}</script><p><em>注：想更多分配概率质量给分子</em></p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-13_17-10-33.png" alt=""></p><p>优化随机游走=最小化$\mathcal{L}$</p><p>但上面两个求和给出复杂度$O(|V|^2)$</p><p>负采样：</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-13_17-10-44.png" alt=""></p><p>$\sigma$为 sigmoid 函数，$n_i$的选取以带偏移的随机方式采样。</p><p><strong>不针对网络中所有结点进行标准化，针对$k$个随机负样本$n_i$归一化</strong></p><p>如何选择负样本？</p><ul><li>采样$k$个负结点，每个具有与其度数成正比的概率</li></ul><p>$k$的选取？</p><ul><li>较高的$k$值会给更可靠的估计</li><li>较高的$k$值会导致更多采样，对负面事件有更高的偏差</li><li>实际上，$k$=5-20</li></ul><p>梯度下降法：</p><ul><li>初始化结点的嵌入$z_i$</li><li>迭代直到收敛<ul><li>对所有$i$，计算$\frac{\partial \mathcal{L}}{\partial z_i}$</li><li>$z_i\leftarrow z_i-\eta \frac{\partial \mathcal{L}}{\partial z_i}$</li></ul></li></ul><p>随机梯度下降：</p><p>以随机方式逼近梯度，不对所有的进行评估，而选择其中的一小部分</p><ul><li>初始化结点的嵌入$z_i$</li><li>迭代直到收敛：$\mathcal{L}^{(u)}=\sum_{v\in N_R(u)}-\log(P(v|\boldsymbol{z_u}))$<ul><li>对一个结点$i$，对所有的$j$计算$\frac{\partial \mathcal{L}^{(i)}}{\partial z_j}$</li><li>$z_j\leftarrow z_j-\eta \frac{\partial \mathcal{L}^{(i)}}{\partial z_j}$</li></ul></li></ul><hr><h5 id="1-2-2-总结"><a href="#1-2-2-总结" class="headerlink" title="1.2.2 总结"></a>1.2.2 总结</h5><ul><li>从图上每个结点开始的固定长度的短随机游走</li><li>对于每个$u$，收集多重集$N_R(u)$</li><li>使用随机梯度优化$\mathcal{L}$，可以使用负采样进行有效近似</li></ul><p><strong>How should we randomly walk？</strong></p><p>最简单的想法：从每个结点开始运行固定长度无偏随机游走</p><p>但这可能太过限制了，推广到 node2vec</p><hr><h4 id="1-3-node2vec"><a href="#1-3-node2vec" class="headerlink" title="1.3 node2vec"></a>1.3 node2vec</h4><p><strong>Goal：</strong>在特征空间中紧密嵌入具有相似网络邻域的节点</p><p><strong>Key observation：</strong>灵活的网络邻域概念会导致更丰富结点嵌入</p><p>构造偏移2阶随机游走为$u$生成邻域$N_R(u)$</p><p><strong>Idea：</strong>使用灵活的有偏随机游走，在局部视图与全局视图之间进行权衡</p><p>来自 DFS 和 BFS 的直觉</p><p>Biased fixed-length random walk $R$ that given a node $u$ generates neighborhood $N_R(u)$</p><p>两个超参数：</p><ul><li><p>Return parameter $p$</p><ul><li>回到上一个结点的概率</li></ul></li><li><p>In-out parameter $q$</p><ul><li>Moving outwards (DFS) vs. moving inwards (BFS)</li><li>Intuitively, $q$ is the ratio of BFS vs. DFS</li></ul></li></ul><p><strong>Biased $2^{nd}$-order random walks explore network neighborhoods</strong></p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-12_23-25-21.png" alt=""></p><p><strong>算法：</strong></p><ul><li>计算随机游走概率</li><li>模拟 $r$  有偏随机游走，长度为 $l$</li><li>使用随机梯度下降优化</li></ul><p>线性时间复杂度，三个步骤可以并行。</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-12_23-31-26.png" alt=""></p><h3 id="2-Embedding-Entire-Graphs"><a href="#2-Embedding-Entire-Graphs" class="headerlink" title="2. Embedding Entire Graphs"></a>2. Embedding Entire Graphs</h3><p><strong>Goal:</strong>embed a subgraph or an entire graph $G$ to $\boldsymbol{z_G}$</p><p><strong>Tasks:</strong> 通过分子分类来预测哪些分子有毒，哪些无毒；图形异常检测</p><h4 id="2-1-Approach-1"><a href="#2-1-Approach-1" class="headerlink" title="2.1 Approach 1"></a>2.1 Approach 1</h4><ul><li>运行标准的结点嵌入</li><li>对所有的嵌入求和或求平均</li></ul><script type="math/tex; mode=display">\boldsymbol{z_G=\sum_{v\in G}z_v}</script><h4 id="2-2-Approach-2"><a href="#2-2-Approach-2" class="headerlink" title="2.2 Approach 2"></a>2.2 Approach 2</h4><p>引入一个虚拟结点来表示整个图或子图，然后运行标注的图嵌入或结点嵌入技术。</p><p>虚拟结点与图或子图中的每个结点相连。</p><h4 id="2-3-Anonymous-Walk-Embeddings"><a href="#2-3-Anonymous-Walk-Embeddings" class="headerlink" title="2.3 Anonymous Walk Embeddings"></a>2.3 Anonymous Walk Embeddings</h4><p>匿名游走不将随机游走表示为它访问的结点序列，而是第一次访问结点的时间序列</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-13_14-32-49.png" alt=""></p><p>随着匿名游走的长度增加，其数目呈指数上升。</p><h5 id="2-3-1-Simple-Use"><a href="#2-3-1-Simple-Use" class="headerlink" title="2.3.1 Simple Use"></a>2.3.1 Simple Use</h5><p>模拟长度为 $l$ 的匿名游走，并记录它们的计数，将图表示为这些游走的概率分布</p><p><strong>Example:</strong></p><ul><li>$l$ = 3</li><li>可以将图表示为 5 维向量，因为有 5 种长度为 3 的匿名游走 $w_i$：111，112，121，122，123</li><li>$Z_G[i]$ 为 $G$ 在匿名游走 $w_i$ 中的概率</li></ul><p><strong>采样匿名游走：</strong>生成独立的 $m$ 个随机游走，再将图表示为这些游走的概率分布</p><p>distribution have error of no more than $\varepsilon$ with prob. less than $\delta$:</p><script type="math/tex; mode=display">m=\lceil\frac{2}{\varepsilon}(\log (2^\eta-2)-\log(\delta))\rceil</script><p> $\eta$ 为长度 $l$ 的匿名游走的种数</p><h5 id="2-3-2-Learn-Walk-Embeddings"><a href="#2-3-2-Learn-Walk-Embeddings" class="headerlink" title="2.3.2 Learn Walk Embeddings"></a>2.3.2 Learn Walk Embeddings</h5><p>学习匿名游走 $w_i$ 的嵌入 $z_i$，将 $Z_G$ 与所有匿名游走 $z_i$ 一起学习</p><p><strong>如何学习？</strong></p><p>要学习的 $z_G$ 为整个图的嵌入</p><p>从结点1出发，使用采样匿名游走</p><p>想法是学习预测在 $\Delta$ 窗口大小下共同发生的游走</p><script type="math/tex; mode=display">\max \sum_{t=\Delta}^{T-\Delta} \log P(w_t|w_{t-\Delta},\dots,w_{t+\Delta},z_G)</script><ul><li>从 $u$ 运行 $T$ 个不同的长度为 $l$ 的随机游走</li></ul><script type="math/tex; mode=display">N_R(u)=\left\{w_1^u,\dots w_T^u\right\}</script><ul><li>学习预测同时出现在 $\Delta$ 大小窗口的游走</li><li>使用匿名游走 $w_i$ 来估计嵌入 $z_i$</li></ul><script type="math/tex; mode=display">\max_{Z,d} \frac{1}{T}\sum_{t=\Delta}^{T-\Delta}\log P(w_t|\left\{w_{t-\Delta},\dots,w_{t+\Delta},z_G\right\})</script><p><strong>$\boldsymbol{z_G}$的使用</strong>：图分类</p><ul><li>点积：$z_{G1}^Tz_{G2}$</li><li>作为神经网络的输入</li></ul><hr><h3 id="3-Summary：How-to-use-Embeddings"><a href="#3-Summary：How-to-use-Embeddings" class="headerlink" title="3. Summary：How to use Embeddings"></a>3. Summary：How to use Embeddings</h3><ul><li><strong>Clustering/community detection:</strong> Cluster points $z_i$</li><li><strong>Node classification:</strong> Predict label of node $i$ based on $z_i$</li><li><strong>Link prediction:</strong> Predict edge $(i,j)$ based on $(z_i,z_j)$<ul><li>Concatenate: $f(z_i,z_j)=g([z_i,z_j])$</li><li>Hadamard: $f(z_i,z_j)=g(z_i * z_j)$</li><li>Sum/Avg: $f(z_i,z_j)=g(z_i+z_j)$</li><li>Distance: $f(z_i,z_j)=g(||z_i-z_j||_2)$</li></ul></li><li><strong>Graph classification:</strong> graph embedding $z_G$ via aggregating node embeddings or anonymous random walks. </li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224W-01</title>
      <link href="posts/277284787/"/>
      <url>posts/277284787/</url>
      
        <content type="html"><![CDATA[<h3 id="1-图机器学习的应用："><a href="#1-图机器学习的应用：" class="headerlink" title="1. 图机器学习的应用："></a>1. 图机器学习的应用：</h3><h4 id="1-1-Node-level"><a href="#1-1-Node-level" class="headerlink" title="1.1 Node-level"></a>1.1 Node-level</h4><ul><li>蛋白质折叠$\rightarrow$AlphaFold</li></ul><h4 id="1-2-Edge-level"><a href="#1-2-Edge-level" class="headerlink" title="1.2 Edge-level"></a>1.2 Edge-level</h4><ul><li>推荐系统</li><li>药物副作用</li></ul><h4 id="1-3-Subgraph-level"><a href="#1-3-Subgraph-level" class="headerlink" title="1.3 Subgraph-level"></a>1.3 Subgraph-level</h4><ul><li>交通预测</li></ul><h4 id="1-4-Graph-level"><a href="#1-4-Graph-level" class="headerlink" title="1.4 Graph-level"></a>1.4 Graph-level</h4><ul><li>药物发现</li><li>分子生成、优化</li><li>物理模拟</li></ul><h3 id="2-图形表示的选择"><a href="#2-图形表示的选择" class="headerlink" title="2. 图形表示的选择"></a>2. 图形表示的选择</h3><ul><li>边、结点</li><li>无向图 vs. 有向图</li><li>结点度数 vs. 出度入度</li><li>二部图 $\rightarrow$ 折叠网络（只使用二部图一侧的结点，如果两个结点至少有一个共同的邻居，则在其之间创建连接，生成两个图）</li><li>邻接矩阵</li><li>稀疏</li><li>边列表  vs. 邻接表</li><li>附加属性：<ul><li>权重</li><li>排名</li><li>类型</li><li>倾向</li></ul></li><li>连通性 vs. 强连通&amp;弱连通</li><li>强连通分量</li></ul><h3 id="3-传统方法"><a href="#3-传统方法" class="headerlink" title="3. 传统方法"></a>3. 传统方法</h3><h4 id="3-1-步骤："><a href="#3-1-步骤：" class="headerlink" title="3.1 步骤："></a>3.1 步骤：</h4><ul><li>获取数据点、结点、链接、图，训练经典机器学习模型</li><li>将这个模型应用于新结点并作出预测</li></ul><h4 id="3-2-特征设计（手工设计特征）"><a href="#3-2-特征设计（手工设计特征）" class="headerlink" title="3.2 特征设计（手工设计特征）"></a>3.2 特征设计（手工设计特征）</h4><ul><li>Node-level</li><li>Link-level</li><li>Graph-level</li></ul><p>着重关注无向图，目标：对一组感兴趣的对象进行预测。</p><p><strong>Given:$G=(V,E)$</strong></p><p><strong>Learn a function:$f:V\rightarrow \mathbb{R}$</strong></p><h4 id="3-3-Node-Level"><a href="#3-3-Node-Level" class="headerlink" title="3.3 Node-Level"></a>3.3 Node-Level</h4><blockquote><p>表述一个结点在网络中的结构与位置</p></blockquote><hr><h5 id="3-3-1-Node-degree"><a href="#3-3-1-Node-degree" class="headerlink" title="3.3.1 Node degree"></a>3.3.1 Node degree</h5><p><strong>Treat all neighboring nodes equally</strong></p><p>度数相同可能无法区分</p><hr><h5 id="3-3-2-Node-centrality（结点中心性度量）"><a href="#3-3-2-Node-centrality（结点中心性度量）" class="headerlink" title="3.3.2 Node centrality（结点中心性度量）"></a>3.3.2 Node centrality（结点中心性度量）</h5><p>试图描述或捕捉图中结点重要性这一概念。$c_v$</p><ul><li><strong>Eigenvector centrality</strong></li></ul><p>A node $v$ is important if surrounded by important neiboring nodes $u \in N(v)$. </p><script type="math/tex; mode=display">c_v=\frac{1}{\lambda}\sum_{u\in N(v)}c_u \Longleftrightarrow\lambda \boldsymbol{c=Ac}</script><p>$\boldsymbol{A}$邻接矩阵，$\boldsymbol{c}$中心性度量的向量，可以得到：</p><p><strong>$\lambda_{max}$ is always positive and unique,the leading eigenvector $c_{max}$ is used for centrality.</strong></p><ul><li><strong>Betweenness centrality</strong></li></ul><p>A node is important if it lies on many shortest paths between other nodes.</p><script type="math/tex; mode=display">c_v=\sum_{s\neq v\neq t}\frac{\#(\mathrm{shortest\  paths\  between}\  s\  \mathrm{and}\  t\  \mathrm{that\  contain}\  v)}{\# (\mathrm{shortest\ paths \ between}\ s\ \mathrm{and}\ t)}</script><ul><li><strong>Closeness centrality</strong></li></ul><p>A node is important if it has small shortest path lengths to all other nodes.</p><script type="math/tex; mode=display">c_v=\frac{1}{\sum_{u\neq v}\mathrm{shortest\ path \ length\ between }\ u\ \mathrm{and}\ v}</script><hr><h5 id="3-3-3-Clustering-coefficient"><a href="#3-3-3-Clustering-coefficient" class="headerlink" title="3.3.3 Clustering coefficient"></a>3.3.3 Clustering coefficient</h5><p>Measures how connected $v’$s neighboring nodes are:</p><script type="math/tex; mode=display">e_v=\frac{\#(\mathrm{edges\ among \ neighboring\ nodes})}{\left(\begin{array}{c}k_v\\2\end{array}\right)}\in \boldsymbol{[0,1]}</script><p>$k_v$是$v$的度数，注意分子为$v$的邻居们之间连了几条边。</p><h5 id="3-3-4-Graphlets"><a href="#3-3-4-Graphlets" class="headerlink" title="3.3.4 Graphlets"></a>3.3.4 Graphlets</h5><p>Clustering coefficient counts the #(triangles) in the ego-network</p><p>ego-network是由结点本身及其邻居诱导的网络。</p><p><strong>We can generalize the above by counting #(pre-spcified subgraphs i.e. graphlets)</strong></p><p><strong>graphlets</strong>: Rooted connected non-isomorphic subgraphs</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-10_23-31-05.png" alt=""></p><p><strong>Graphlet Degree Vector(GDV)</strong>:Graphlet-base features for nodes</p><ul><li>Degree 统计一个点接触的边数</li><li>Clustering coefficient 统计一个点接触的三角形数</li><li>GDV 统计一个点接触的graphlets的个数</li></ul><p>GDV 提供了一个结点的局部网络拓扑的度量，我们可以用73维的向量描述每一个结点</p><hr><h5 id="3-3-5-总结"><a href="#3-3-5-总结" class="headerlink" title="3.3.5 总结"></a>3.3.5 总结</h5><p>分类：</p><ul><li>基于重要性的特征，用于预测图中结点的重要性：识别社交网络中的名人用户<ul><li>Node degree</li><li>Node centrality measures</li></ul></li><li>基于结构的特征，用于预测特定的结点角色：预测蛋白质功能<ul><li>Node degree</li><li>Clustering coefficient</li><li>Graphlet Degree Vector</li></ul></li></ul><h4 id="3-4-Link-Level"><a href="#3-4-Link-Level" class="headerlink" title="3.4 Link-Level"></a>3.4 Link-Level</h4><p>任务是根据网络中现有的链接预测新的链接，所以要评估所有尚未链接的结点对，给出一个排名，前K个为预测的结果。关键就是为一对结点设计特征。</p><p>两种预测问题的表述：</p><ul><li>链接随机丢失（静态网络）：随机移除一些链接，然后尝试用机器学习算法将它们预测回来</li><li>随着时间推移预测链接（随时间发展的网络）：给定直到时间$t_0’$的图$G[t_0,t_0’]$，输出一个排名列表$L$，包含预测会出现在$G[t_1,t_1’]$但不在$G[t_0,t_0’]$中的链接。<ul><li>$n=|E_{new}|$: <script type="math/tex">\#\  \mathrm{new\ edges\ that\ appear\ during\ the\ test\ period\ } [t_1,t_1']</script></li><li>$\mathrm{Take\ top\ }n\ \mathrm{elements\ of\ }L\ \mathrm{and\ count\ correct\ edges}$</li></ul></li></ul><blockquote><p><strong>方法论</strong></p><ul><li>对于结点对$(x,y)$，计算分数$c(x,y)$</li><li>依据$c(x,y)$进行降序排列</li><li>预测最高的$n$个结点对作为新链接</li><li>观察哪些链接是实际上出现在$G[t_1,t_1’]$中的</li></ul></blockquote><h5 id="3-4-1-Distance-based-feature"><a href="#3-4-1-Distance-based-feature" class="headerlink" title="3.4.1 Distance-based feature"></a>3.4.1 Distance-based feature</h5><p>两个结点的最短路径距离，显然这种特征没有捕捉邻域重叠程度或链接强度</p><h5 id="3-4-2-Local-neighborhood-overlap"><a href="#3-4-2-Local-neighborhood-overlap" class="headerlink" title="3.4.2 Local neighborhood overlap"></a>3.4.2 Local neighborhood overlap</h5><p><strong>Captures # neighboring nodes shared between two nodes $v1$ and $v2$</strong></p><ul><li>Common neighbors:$|N(v_1)\cap N(v_2)|$</li><li>Jaccard’s coefficient:$\frac{|N(v_1)\cap N(v_2)|}{|N(v_1)\cup N(v_2)|}$</li><li>Adamic-Adar index:$\sum_{u\in N(v_1)\cap N(v_2)}\frac{1}{\mathrm{log}(k_u)}$</li></ul><p><strong>Limitaton:</strong>两个结点没有共同邻居时度量总为0，但它们在未来仍有可能相连</p><h5 id="3-4-3-Global-neighborhood-overlap"><a href="#3-4-3-Global-neighborhood-overlap" class="headerlink" title="3.4.3 Global neighborhood overlap"></a>3.4.3 Global neighborhood overlap</h5><p><strong>Katz index</strong>: 统计给定的结点对之间路径的所有不同长度</p><ul><li>计算两个结点之间的路径数：使用邻接矩阵的幂$A_{uv}^l$计算$u,v$结点之间长度为$l$的路径数</li></ul><p><strong>Katz index</strong>可表示为：</p><script type="math/tex; mode=display">S_{v_1v_2}=\sum_{l=1}^{\infty}\beta^lA_{v_1v_2}^l</script><p>$\beta\in(0,1)$给越长的路径设定越低的重要性</p><p>闭式计算：</p><script type="math/tex; mode=display">S=\sum_{i=1}^{\infty}\beta^iA^i=(I-\beta A)^{-1}-I</script><h4 id="3-5-Graph-Level"><a href="#3-5-Graph-Level" class="headerlink" title="3.5 Graph-Level"></a>3.5 Graph-Level</h4><p><strong>Goal</strong>:features that characterize the structure of an entire graph</p><p><strong>Kernel methods</strong>:Design kernels instead of feature vectors</p><blockquote><p>Quick introduction:</p><ul><li>Kernal $K(G,G’)\in \mathbb{R}$ measures similarity b/w data</li><li>Kernel matrix $\boldsymbol{K}=(K(G,G’))_{G,G’}$must be positive semidefinite</li><li>There exists a feature representation $\phi(\cdot)$ such that $K(G,G’)=\phi(G)^T\phi(G’)$</li><li>Once the kernel is defined,off-the-shelf ML model such as kernel SVM can be used to make predictions</li></ul></blockquote><p>举例：</p><ul><li>Graphlet Kernel</li><li>Weisfeiler-Lehman Kernel</li><li>Random-walk Kernel</li><li>Shortest-path graph Kernel</li></ul><h5 id="3-5-1-Goal-Design-graph-feature-vector-boldsymbol-phi-G"><a href="#3-5-1-Goal-Design-graph-feature-vector-boldsymbol-phi-G" class="headerlink" title="3.5.1 Goal:Design graph feature vector $\boldsymbol{\phi(G)}$"></a>3.5.1 Goal:Design graph feature vector $\boldsymbol{\phi(G)}$</h5><p><strong>Key idea:</strong>Bags-of-Words for a graph$\color{Red}{(图的词袋表示)}$</p><p>首先不能简单将结点视为单词，相同结点数目输出相同。</p><p>使用 Bags of node degrees 就能为不同的图生成不同特征。</p><p><strong>Graphlet Kernel</strong>与<strong>Weisfeiler-Lehman(WL) Kernel</strong> 都采用这种Bag-of-*表示方法，*是比结点度数更为复杂的特征。</p><h5 id="3-5-2-Graphlet-Features"><a href="#3-5-2-Graphlet-Features" class="headerlink" title="3.5.2 Graphlet Features"></a>3.5.2 Graphlet Features</h5><p><strong>Key idea:</strong>Count the number of different graphlets in a graph.</p><p><em>注：这里定义的 graphlets 与 node-level 中的有所不同</em></p><ul><li>Nodes in graphlets here do not need to be connected</li><li>The graphlets here are not rooted</li></ul><p>Given graph $G$, a graphlet list $\mathcal{G_k}=(g_1,\dots,g_{n_k})$,define the graphlet count vector $f_G\in \mathbb{R}^{n_k}$ as</p><script type="math/tex; mode=display">(f_G)_i= \#(g_i \subseteq G) \ for \ i= 1,\dots n_k</script><script type="math/tex; mode=display">K(G,G')=\boldsymbol{f_G^Tf_{G^{\boldsymbol{'}}}}</script><p><strong>Problem:</strong>$G$ 和 $G^{‘}$大小不同时，会引起值的偏向。</p><p><strong>Solution:</strong>对特征向量归一化</p><script type="math/tex; mode=display">\boldsymbol{h_G=\frac{f_G}{Sum(f_G)}}</script><script type="math/tex; mode=display">K(G,G')=\boldsymbol{h_G^Th_{G^{\boldsymbol{'}}}}</script><p><strong>Limitations:</strong>统计graphlets非常昂贵</p><h5 id="3-5-3-WL-Kernel"><a href="#3-5-3-WL-Kernel" class="headerlink" title="3.5.3 WL Kernel"></a>3.5.3 WL Kernel</h5><p><strong>Goal:</strong>设计更加有效的办法</p><p><strong>Idea:</strong>使用邻域结构迭代地丰富结点词典，推广Bag of node degrees，从一跳邻域信息到多跳邻域信息。</p><p><strong>Algorithm:</strong>Color refinement（颜色细化）或 Weisfeiler-Lehman图同构检验</p><p><strong>Given:</strong>带有一组结点$V$，为每个结点$v$分配初始颜色$c^{(0)}(v)$，迭代</p><script type="math/tex; mode=display">c^{(k+1)}(v)=\boldsymbol{HASH}(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right\})</script><p><strong>HASH</strong>将不同输入映射到不同颜色</p><p>经过$K$步，得到的$c^{(K)}(v)$总结了K跳邻域的结构。</p><p>在颜色细分后，WL Kernel统计给定颜色的结点数，得到一个向量</p><p>$K(G,G’)$仍然定义为向量的点积。</p><p>时间复杂度关于边数是线性的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN综述</title>
      <link href="posts/3892482635/"/>
      <url>posts/3892482635/</url>
      
        <content type="html"><![CDATA[<h3 id="Spectral-methods-amp-Spatial-methods"><a href="#Spectral-methods-amp-Spatial-methods" class="headerlink" title="Spectral methods &amp; Spatial methods"></a>Spectral methods &amp; Spatial methods</h3><ul><li>谱方法在谱域里定义 ，通过Fourier变化到谱域，在谱域实现卷积定义;</li><li>空间方法在结点域定义卷积，由于结点邻域大小不同，实现参数共享困难，关键就是参数共享。</li></ul><h2 id="1-Spectral-methods"><a href="#1-Spectral-methods" class="headerlink" title="1. Spectral methods"></a>1. Spectral methods</h2><h4 id="1-1-1-输入"><a href="#1-1-1-输入" class="headerlink" title="1.1.1 输入"></a>1.1.1 输入</h4><p>图$\boldsymbol{G = (V,E,W)}$</p><p>$\boldsymbol{V}$为点集，$\boldsymbol{n=|V|}$，$\boldsymbol{E}$为边集，$\boldsymbol{W \in R^{n\times n}}$权重集合。</p><p>每一个点都有一个$\boldsymbol{d}$维特征，所以有$\boldsymbol{X \in  R^{n\times d}}$的特征矩阵，每一列可以看做定义在这些结点上的信号。</p><h4 id="1-1-2-图上的Laplacian"><a href="#1-1-2-图上的Laplacian" class="headerlink" title="1.1.2 图上的Laplacian"></a>1.1.2 图上的Laplacian</h4><p>对图做谱分析的基本工具，定义了图上的导数，刻画了信号的平滑程度。</p><ul><li>$\boldsymbol{L=D-W}$</li><li>归一化版本 $\boldsymbol{L_{norm}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}=I-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}}$，将主对角线全变为1</li><li>$L$为实对称矩阵，可以用正交矩阵进行正交相似对角化。</li></ul><script type="math/tex; mode=display">L=U\Lambda U^T</script><ul><li>以$L$为系数矩阵的二次型，图的总变差：</li></ul><script type="math/tex; mode=display">x^TLx=x^TU\Lambda U^Tx=\sum_{i=1}^{n}\sum_{j\in N(i)}x_i(x_i-x_j)=\sum_{e_{ij}}(x_i-x_j)^2</script><ul><li>$L$具有以下性质：<ul><li>每行元素之和为零，至少有一个特征值为0，对应特征向量$u_0=\frac{[1,1,\dots1]^T}{\sqrt{n}}$,$\textbf{L}u_0=0u_0$</li><li>特征值都大于等于0，归一化拉普拉斯矩阵特征值区间为[0,2]</li></ul></li></ul><p>图微分的定义：</p><ul><li>第$i$个结点处，一阶偏导</li></ul><script type="math/tex; mode=display">\frac{\partial \boldsymbol{x}}{\partial e(i,j)}=\sqrt{W_{i,j}}(x_j-x_i)</script><ul><li>第$i$个结点处，梯度是一个向量</li></ul><script type="math/tex; mode=display">\nabla_i \boldsymbol{x}=\left\{\frac{\partial \boldsymbol{x}}{\partial e(i,j)}|j\in N(i)\right\}</script><ul><li>图的局部平滑度</li></ul><script type="math/tex; mode=display">||\nabla_i \boldsymbol{x}||=\left[{\sum_{j\in N(i)}\left(\frac{\partial \boldsymbol{x}}{\partial e(i,j)}\right)^2}\right]^{\frac{1}{2}}=\left[{\sum_{j\in N(i)}W_{ij}\left(x_j-x_i\right)^2}\right]^{\frac{1}{2}}</script><ul><li>全局平滑度：离散p-狄利克雷型</li></ul><script type="math/tex; mode=display">S_p(x)=\frac{1}{p}\sum_{i}^{V}||\nabla_i \boldsymbol{x}||_2^p</script><ul><li>$p=2$时，</li></ul><script type="math/tex; mode=display">S_2(x)=\frac{1}{2}\sum_{i\in V}\sum_{j\in N(i)}W_{i,j}(x_j-x_i)^2=\sum_{e(i,j)\in E}W_{i,j}(x_j-x_i)^2</script><h4 id="1-1-3-Fourier基"><a href="#1-1-3-Fourier基" class="headerlink" title="1.1.3 Fourier基"></a>1.1.3 Fourier基</h4><p>$\boldsymbol{n}$个结点，每个结点有一个取值，图上的一个信号就是$\boldsymbol{n}$维向量，要变换到新的域里，就需要一组基，这就是$\boldsymbol{L}$的$\boldsymbol{n}$个特征向量，彼此正交。取基为特征向量$\left\{ u_l \right\}_{l=1}^n$，由非负特征值$\left\{ \lambda_l \right\}_{l=1}^n$排序 。可以证明更小的特征值对应的特征向量更平滑，因此我们可以将拉普拉斯矩阵的特征值等价成频率。低频特征向量变化缓慢，其直观的效果是两个被大权重边相连的节点它们的信号值十分接近；高频特征向量则剧烈振荡，相邻节点的信号值差异较大。</p><p>$L$的正交相似对角化：</p><script type="math/tex; mode=display">L=U\Lambda U^{T}</script><p>$\boldsymbol{U=\left[u_1,\dots ,u_n\right]}$，$\boldsymbol{\Lambda=diag\left(\left[\lambda_1,\dots ,\lambda_n\right] \right)}$。</p><p>对于一个信号$x\in R^n$，Fourier变换定义为</p><script type="math/tex; mode=display">\hat{x}=U^{T}x</script><p>逆变换</p><script type="math/tex; mode=display">x=U\hat{x}</script><p>总变差</p><script type="math/tex; mode=display">x^TLx=x^TU\Lambda U^Tx=\hat x^T\Lambda\hat{x}=\sum_{i=1}^{n}\lambda_i\hat{x_i}^2</script><h4 id="1-1-4-卷积"><a href="#1-1-4-卷积" class="headerlink" title="1.1.4 卷积"></a>1.1.4 卷积</h4><p>两个信号的卷积可以看作Fourier变换后的点积。</p><script type="math/tex; mode=display">x \ast_G y =U((U^Tx)\odot(U^Ty))</script><p>这里的卷积核为$U^Ty$</p><p>令$U^Ty=[\theta_0,\dots,\theta_{n-1}]^{T}$，将其变为矩阵$g_\theta=diag([\theta_0,\dots,\theta_{n-1}])$</p><p> $g_\theta$是有n个参数的对角阵，参数是自由的、需要学习的参数</p><p>上式可变化为</p><script type="math/tex; mode=display">x \ast_G y = Ug_\theta U^Tx</script><hr><h3 id="1-2-Spectral-Graph-CNN"><a href="#1-2-Spectral-Graph-CNN" class="headerlink" title="1.2 Spectral Graph CNN"></a>1.2 Spectral Graph CNN</h3><script type="math/tex; mode=display">x_{k+1,j}=h\left(\sum_{i=1}^{f_k}UF_{k,i,j}U^Tx_{k,i}\right),j=1,\dots,f_{k+1}</script><p>$x_{k,i}$是第k层的信号，$F_{k,i,j}$是第k层的核，$h(\cdot)$为卷积运算。</p><h4 id="1-2-1-Spectral-Graph-CNN的短板"><a href="#1-2-1-Spectral-Graph-CNN的短板" class="headerlink" title="1.2.1 Spectral Graph CNN的短板"></a>1.2.1 Spectral Graph CNN的短板</h4><ul><li>依赖于Laplacian矩阵分解，非常复杂$\boldsymbol O\left(n^3\right)$</li><li>高计算代价，稠密的矩阵乘$\boldsymbol x$复杂度$\boldsymbol O\left(n^2\right)$</li><li>在结点域是非局部的(not localized)</li></ul><hr><h3 id="1-3-ChebyNet"><a href="#1-3-ChebyNet" class="headerlink" title="1.3 ChebyNet"></a>1.3 ChebyNet</h3><p>使用多项式函数对卷积核作参数化</p><p>将$g_\theta$变为</p><script type="math/tex; mode=display">g_{\beta}\left(\Lambda\right)=\sum_{k=0}^{K-1}\beta_k\Lambda^k, \Lambda=diag(\lambda_1,\dots,\lambda_n)</script><p>这样</p><script type="math/tex; mode=display">x \ast_G y = Ug_\beta(\Lambda) U^Tx=\sum_{k=0}^{K-1}\beta_kL^kx</script><ul><li>不需要特征分解</li><li>L稀疏，降低复杂度为$\boldsymbol{O(|E|)}$</li><li>L为局部化函数，卷积只受k跳邻居邻居影响</li></ul><h4 id="1-3-1-切比雪夫多项式"><a href="#1-3-1-切比雪夫多项式" class="headerlink" title="1.3.1 切比雪夫多项式"></a>1.3.1 切比雪夫多项式</h4><p>引入切比雪夫多项式近似$L^k$,$T_k(x)$为k阶切比雪夫多项式</p><script type="math/tex; mode=display">g_{\beta}\left(\Lambda\right)=\sum_{k=0}^{K-1}\beta_kT_k(\tilde\Lambda), \tilde\Lambda=\frac{2}{\lambda_{max}}\Lambda-I_N</script><p>切比雪夫多项式的输入在$[-1,1]$之间。</p><hr><h3 id="1-4-Graph-wavelet-nerual-network"><a href="#1-4-Graph-wavelet-nerual-network" class="headerlink" title="1.4 Graph wavelet nerual network"></a>1.4 Graph wavelet nerual network</h3><p>ChebyNet通过约束卷积取值空间，限制为$\Lambda$的多项式函数。</p><p>之前的问题主要来自$\boldsymbol{U}$ 的性质不够好，所以采用小波基来代替Fourier基。</p><p>小波基具有以下性质：</p><ul><li>稀疏</li><li>局部化</li><li>低计算代价</li></ul><div class="table-container"><table><thead><tr><th></th><th>$Fourier$</th><th>$Wavelet$</th></tr></thead><tbody><tr><td>$basis$</td><td>$U$</td><td>$\psi_s=Ue^{\lambda s}U^{T}$</td></tr><tr><td>$transform$</td><td>$\hat{x}=U^Tx$</td><td>$x^{\ast}=\psi_s^{-1}x$</td></tr><tr><td>$inverse \ transform$</td><td>$x=U\hat{x}$</td><td>$x=\psi_Sx^{\ast}$</td></tr><tr><td>$convolution$</td><td>$x \ast_G y =U((U^Ty)\odot(U^Tx))$</td><td>$x \ast_G y = \psi_s((\psi_s^{-1}y)\odot(\psi_s^{-1}x))$</td></tr><tr><td>$neural\ network$</td><td>$x_{k+1,j}=h\left(\sum_{i=1}^{p}UF_{k,i,j}U^Tx_{k,i}\right),\\j=1,\dots,q$</td><td>$x_{k+1,j}=h\left(\sum_{i=1}^{p}\psi_s F_{k,i,j}\psi_s ^{-1}x_{k,i}\right),\\j=1,\dots,q$</td></tr></tbody></table></div><p>复杂度：$O(n \ast p \ast q)$难以接受</p><h4 id="1-4-1-Key-idea"><a href="#1-4-1-Key-idea" class="headerlink" title="1.4.1 Key idea"></a>1.4.1 Key idea</h4><p>图卷积与图的特征变换不能同时进行，否则参数个数非常大，拆分为：</p><script type="math/tex; mode=display">x_{k+1,j}=h\left(\sum_{i=1}^{p}\psi_s F_{k,i,j}\psi_s ^{-1}x_{k,i}\right),\\j=1,\dots,q</script><script type="math/tex; mode=display">\Downarrow</script><script type="math/tex; mode=display">\boldsymbol{Feature\ transformation}\\y_{k,j}=\sum_{i=1}^pT_{ji}x_{k,i},T \in R^{q\times p}with \ p \ast q \ parameters</script><script type="math/tex; mode=display">+</script><script type="math/tex; mode=display">\boldsymbol{Graph \ convolution}\\x_{k+1,j}=h\left(\psi_sF_k\psi_s^{-1}y_{k,j}\right),F_k\  is \ a \ diagonal\ matrix \ with\  n \ parameters</script><p>复杂度：$O(n + p \ast q)$</p><p><strong>谱方法为空间方法的特例</strong></p><hr><h2 id="2-Spatial-methods"><a href="#2-Spatial-methods" class="headerlink" title="2. Spatial methods"></a>2. Spatial methods</h2><h3 id="2-1-通过类比"><a href="#2-1-通过类比" class="headerlink" title="2.1 通过类比"></a>2.1 通过类比</h3><ol><li><strong>确定邻居</strong></li><li>给邻居结点编序</li><li>参数共享</li></ol><p>对每一个结点，为它选择固定数量的邻居，定义一个邻近度的度量。</p><h3 id="2-2-GraphSAGE"><a href="#2-2-GraphSAGE" class="headerlink" title="2.2 GraphSAGE"></a>2.2 GraphSAGE</h3><p>选固定的个数，直接随机选择得了，用随机行走选固定个数。</p><ul><li><p>Sampling neighbors</p></li><li><p>Aggregating neighbors</p></li></ul><script type="math/tex; mode=display">a_v^{(k)}=AGGREGATE^{(k)} \left(\left\{h_u^{(k-1)}:u \in \mathcal{N(v)} \right\}\right)\\h_v^{(k)}=COMBINE^{(k)}\left(h_v^{(k-1)},a_v^{(k)}\right)\\</script><script type="math/tex; mode=display">\color{Red}{Aggregate\ the \ information\ of \ neighboring\\ nodes\ to\ update\ the\ representation\ of\ center\ node.}</script><h3 id="2-3-GCN"><a href="#2-3-GCN" class="headerlink" title="2.3 GCN"></a>2.3 GCN</h3><p>ChebyNet的简化版本，但已经是空间方法了,进一步选取$K=2$，$\lambda_{max}=2$，。</p><ul><li>通过正则化Laplacian矩阵聚合邻域信息</li><li>由特征变化共享参数</li></ul><script type="math/tex; mode=display">g_\theta \ast x \approx \theta_0+\theta_1(L-I_N)x=\theta_0x-\theta_1D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}}x</script><p>在实际训练过程中，我们需要规范化参数来避免过拟合，所以我们令 $\theta=\theta_0^{‘}=-\theta_1^{‘}$</p><script type="math/tex; mode=display">g_\theta \ast x \approx \theta (I_N+ D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}})x</script><p>实际上$I_N+ D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}}$的特征值在$[0,2]$之间，需要再次归一化。</p><script type="math/tex; mode=display">I_N+ D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}}\to \tilde D^{-{\frac{1}{2}}}\tilde A\tilde D^{-{\frac{1}{2}}},\tilde D_{ii}=\sum_j \tilde{A_{ij}},\tilde{A}=A+I_N</script><p>那么对于输入结点向量$X \in R^{N \times C}$，其中 N 为节点数，C 为节点的特征向量维度，我们有：</p><script type="math/tex; mode=display">Z=\tilde D^{-{\frac{1}{2}}}\tilde A\tilde D^{-{\frac{1}{2}}}X\Theta</script><p>$\Theta \in R^{C\times F}$是滤波器的参数矩阵，$Z\in R^{N \times F}$ 是卷积后的信号矩阵，时间复杂度$O(|E|FC)$</p><p>多层图卷积网络：</p><script type="math/tex; mode=display">H^{(l+1)}=\sigma(\tilde D^{-{\frac{1}{2}}}\tilde A\tilde D^{-{\frac{1}{2}}}H^{(l)}W^{(l)})</script><p>$W^{(l)}$为第$l$层权重矩阵，$\sigma(\cdot)$是激活函数，$H^{(l)}\in R^{N \times D}$是第$l$层激活函数，$H^{(0)}=X$</p><p>至于模型：</p><script type="math/tex; mode=display">Z=f(X,A)=softmax(\hat{A}ReLU(\hat AXW^{(0)})W^{(1)})</script><p>$W^{(0)}\in R^{C\times H}$从输入层到隐藏层，$W^{(1)}\in R^{H\times F}$从隐藏层到输出层</p><p>代价函数：</p><script type="math/tex; mode=display">L=-\sum_{l\in y_L}\sum_{f=1}^{F}Y_{lf}\ ln\ Z_{lf}</script><h3 id="2-4-GAT"><a href="#2-4-GAT" class="headerlink" title="2.4 GAT"></a>2.4 GAT</h3><p>GCN没有卷积、参数共享，还需要一个$\vec{a}$，是用一个结点收自己邻域信息时要共享的参数，是Self-Attention。</p><p>自己做完特征变换，邻居做完特征变换，然后组合一下，过一个Attention机制，得到一个权重。</p><script type="math/tex; mode=display">\alpha_{ij}=\frac{\exp(\mathrm{LeakyReLU}(\vec{a}^{T}[W\vec{h_i}||W\vec{h_j}]))}{\sum_{k\in \mathcal{N}_i}\exp(\mathrm{LeakyReLU}(\vec{a}^{T}[W\vec{h_i}||W\vec{h_j}]))}</script><p>$W$为特征变换参数，$\vec{a}$为注意力机制参数</p><h3 id="2-5-MoNet"><a href="#2-5-MoNet" class="headerlink" title="2.5 MoNet"></a>2.5 MoNet</h3><p>空间方法的一般框架。</p><p>定义各种各样的核函数，目的是为了度量目标结点与其他结点的相似度，卷积核则是这些核函数的权重。</p><script type="math/tex; mode=display">(f \ast g)=\sum_{j=1}^{J}g_jD_j(x)f</script><h2 id="3-Spectral-methods-vs-Spatial-methods"><a href="#3-Spectral-methods-vs-Spatial-methods" class="headerlink" title="3. Spectral methods vs. Spatial methods"></a>3. Spectral methods vs. Spatial methods</h2><h3 id="3-1-谱方法是空间方法的特例"><a href="#3-1-谱方法是空间方法的特例" class="headerlink" title="3.1 谱方法是空间方法的特例"></a>3.1 谱方法是空间方法的特例</h3><p>谱方法需要显式定义卷积核，而空间方法不需要。</p><script type="math/tex; mode=display">(f \ast g)=\sum_{j=1}^{J}g_jD_j(x)f</script><p>$D_j(x)$为核函数</p><h3 id="3-2-Spectral-methods"><a href="#3-2-Spectral-methods" class="headerlink" title="3.2 Spectral methods"></a>3.2 Spectral methods</h3><ul><li>Spectral CNN</li></ul><script type="math/tex; mode=display">y=Ug_\theta U^T x=(\theta_1 {\color{Red}{u_1u_1^T}}+\dots +\theta_n {\color{Red}{u_nu_n^T}})</script><ul><li>ChebyNet</li></ul><script type="math/tex; mode=display">y=(\theta_0{\color{Red}I}+\theta_1{\color{Red}L}+\dots+\theta_{K-1}{\color{Red}L^{K-1}})x</script><ul><li>GCN</li></ul><script type="math/tex; mode=display">y=\theta({\color{Red}I}-{\color{Red}L})x</script><h3 id="3-3-基本滤波器"><a href="#3-3-基本滤波器" class="headerlink" title="3.3 基本滤波器"></a>3.3 基本滤波器</h3><p>信号$\boldsymbol{x}$在图上的平滑程度被定义为</p><script type="math/tex; mode=display">x^TLx=\sum_{(u,v)\in E}A_{uv}\left(\frac{x_u}{\sqrt{d_u}}-\frac{x_v}{\sqrt{d_v}}\right)^2</script><p>特征值$\lambda_i=u_i^TLu_i$可以被视为特征向量$u_i$的频率。</p><p>每一个$u_iu_i^T$是一个基本的滤波器，只让频率$\lambda_i$的信号通过。</p><p>Spectral CNN恰是这些滤波器的线性组合。</p><p>ChebyNet的$L^k$是基础滤波器带有特殊系数$\left\{\lambda_i^k\right\}_{i=1}^n$的线性组合。</p><p>即认为信号频率越高，权重越大，实际上是对高频信号的加强，然而高频信号不体现节点分类的平滑性。</p><p>GCN之所以表现更好，是因为只考虑$k=0$和$k=1$，是对ChebyNet的低阶近似。</p><h3 id="3-4-GraphHeat"><a href="#3-4-GraphHeat" class="headerlink" title="3.4 GraphHeat"></a>3.4 GraphHeat</h3><p>设计低通滤波器</p><p>$\left\{e^{-skL}\right\}$，$e^{-sL}$是热核，通过图上热扩散定义相似度：</p><script type="math/tex; mode=display">e^{-sL}=Ue^{-s\Lambda}U^T,\Lambda=diag(\lambda_1,\dots,\lambda_n)</script><p>频率高的信号有一个$e$的指数衰减过程，$u_iu_i^T$的参数是$e^{-s\lambda_i}$</p><h2 id="4-Graph-Pooling"><a href="#4-Graph-Pooling" class="headerlink" title="4. Graph Pooling"></a>4. Graph Pooling</h2><p>早期图上任务关于结点级别，对应图像的像素级别，不需要pooling。</p><h3 id="4-1-Graph-coarsening"><a href="#4-1-Graph-coarsening" class="headerlink" title="4.1 Graph coarsening"></a>4.1 Graph coarsening</h3><p>类似下采样，对图做聚类，最后将每个聚类变成超级结点。</p><h3 id="4-2-Node-selection"><a href="#4-2-Node-selection" class="headerlink" title="4.2 Node selection"></a>4.2 Node selection</h3><p>每做一层厚，选一部分结点作为代表，需要量化结点重要性的度量。</p><h2 id="5-Questions"><a href="#5-Questions" class="headerlink" title="5. Questions"></a>5. Questions</h2><ul><li><strong>Does structure matters?</strong></li><li><strong>Context representation?</strong></li><li><strong>Future applications?</strong></li></ul><hr><h3 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h3><ol><li><a href="https://www.bilibili.com/video/BV1ta4y1t7EK?from=search&amp;seid=18041333950604110050">https://www.bilibili.com/video/BV1ta4y1t7EK?from=search&amp;seid=18041333950604110050</a></li><li><a href="https://zhuanlan.zhihu.com/p/120311352">https://zhuanlan.zhihu.com/p/120311352</a></li><li><a href="https://zhuanlan.zhihu.com/p/290755442">https://zhuanlan.zhihu.com/p/290755442</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="posts/4278564287/"/>
      <url>posts/4278564287/</url>
      
        <content type="html"><![CDATA[<h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><p>代码测试：</p><pre class="line-numbers language-py" data-language="py"><code class="language-py">print("Hello")print("this is second.")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>引用测试：</p><blockquote><p>这是一条引用</p></blockquote><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><ul><li>哈哈</li><li>嘿嘿</li></ul><script type="math/tex; mode=display">lim_{1\to+\infty}P(|\frac{1}{n}\sum_i^nX_i-\mu|<\epsilon)=1, i=1,...,n</script><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/images.png" alt=""></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="posts/1243066710/"/>
      <url>posts/1243066710/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
