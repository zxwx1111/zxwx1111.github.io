<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>cs224w-03</title>
      <link href="posts/624699732/"/>
      <url>posts/624699732/</url>
      
        <content type="html"><![CDATA[<blockquote><p> Graph as Matrix: PageRank, Random Walks and Embeddings</p></blockquote><p>Web as a directed graph.</p><p>所有的网页的重要性并不是相等的，网络图连通性存在如此大的多样性，那我们可以使用网络图链接结构来给页面排名吗？</p><p><strong>链接分析方法（Link Analysis approaches）</strong></p><ul><li>PageRank</li><li>Personalized PageRank (PPR)</li><li>Random Walk with Restarts</li></ul><hr><h3 id="1-PageRank"><a href="#1-PageRank" class="headerlink" title="1. PageRank"></a>1. PageRank</h3><h4 id="1-1-Links-as-votes-Page-is-more-important-if-it-has-more-links"><a href="#1-1-Links-as-votes-Page-is-more-important-if-it-has-more-links" class="headerlink" title="1.1 Links as votes: Page is more important if it has more links"></a>1.1 <strong>Links as votes:</strong> Page is more important if it has more links</h4><ul><li>Links from important pages count more</li><li>Recursive question</li></ul><blockquote><p><strong>Flow： </strong></p><p>if page $i$ with importance $r_i$ has $d_i$ out links, each link gets $r_i/d_i$ votes</p><p>$j$‘s own importance $r_j$ is the sum of the votes on its <strong>in-links</strong></p></blockquote><script type="math/tex; mode=display">r_j=\sum_{i\rightarrow j}\frac{r_i}{d_i}</script><p>$d_i$ is out-degree of node $i$</p><p><strong>随机邻接矩阵$M:$</strong></p><p>Let page $j$ have $d_j$ out-links, if $j \rightarrow i$, then $M_{ij}=\frac{1}{d_j}$, Columns sum to 1</p><p><strong>秩向量 $r:$</strong> An entry per page</p><p>$r_i$ 为页面 $i$ 的重要性，$\sum_ir_i=1$，可以视为一个概率分布。</p><p>那么流动方程可以写成：</p><script type="math/tex; mode=display">r=M\cdot r</script><h4 id="1-2-Connection-to-Random-Walk"><a href="#1-2-Connection-to-Random-Walk" class="headerlink" title="1.2 Connection to Random Walk"></a>1.2 Connection to Random Walk</h4><p>假设一个随机网页浏览者：</p><ul><li>在时间 $t$，浏览者在一些 $i$ 页面上</li><li>在时间 $t+1$，浏览者顺着 $i$ 的外链均匀随机访问导航到 $j$</li><li>过程无限重复</li></ul><p>令：</p><ul><li>$p(t)$ 是一个向量，第 $i$ 个坐标为时间 $t$ 浏览者在页面 $j$ 的概率</li><li>$p(t)$ 是页面上的概率分布</li></ul><p>同样：</p><script type="math/tex; mode=display">r_j=\sum_{i\rightarrow j}\frac{r_i}{d_{out}(i)}</script><script type="math/tex; mode=display">p(t+1)=M\cdot p(t)</script><p>那么如果：$p(t+1)=M \cdot p(t)=p(t)$，那么 $p(t)$ 为随机游走的平稳分布</p><p>即 $r=M \cdot r$ 中的 $r$ 为随机游走的平稳分布</p><p>考虑特征值：</p><script type="math/tex; mode=display">1\cdot r=M \cdot r</script><p>即秩向量 $r$ 是随机邻接矩阵 $M$ 特征值为1的特征向量。</p><blockquote><p><strong>PageRank = Limiting distribution = principal eigenvector of M</strong></p></blockquote><h4 id="1-3-How-to-solve"><a href="#1-3-How-to-solve" class="headerlink" title="1.3 How to solve?"></a>1.3 How to solve?</h4><h5 id="1-3-1-幂迭代"><a href="#1-3-1-幂迭代" class="headerlink" title="1.3.1 幂迭代"></a>1.3.1 <strong>幂迭代</strong></h5><ul><li>给每个结点分配初始秩</li><li>迭代到收敛 $\sum_i|r_i^{t+1}-r_i^t| &lt; \varepsilon$</li></ul><script type="math/tex; mode=display">r_j^{(t+1)}=\sum_{i \rightarrow j}\frac{r_i^{(t)}}{d_i}</script><h5 id="1-3-2-Power-iteration"><a href="#1-3-2-Power-iteration" class="headerlink" title="1.3.2 Power iteration"></a>1.3.2 <strong>Power iteration</strong></h5><ul><li>initialize: $r^0=[1/N,\dots,1/N]^T$</li><li>iterate: $r^{(t+1)}=M\cdot r^t$ <ul><li>$r_j^{(t+1)}=\sum_{i \rightarrow j}\frac{r_i^{(t)}}{d_i}$</li></ul></li><li>Stop when $|r^{(t+1)}-r^t|_1&lt;\varepsilon$</li></ul><h5 id="1-3-3-Two-problems"><a href="#1-3-3-Two-problems" class="headerlink" title="1.3.3 Two problems:"></a>1.3.3 <strong>Two problems:</strong></h5><ul><li><strong>Spider traps (all out-links are within the group)</strong></li></ul><p>Eventually spider traps absorb all importance</p><p>比如 $a$ 指向 $b$, $b$ 指向自己</p><ul><li><strong>Some pages are dead ends (have no out-links)</strong></li></ul><p>Such pages cause importance to leak out</p><p>比如 $a$ 指向 $b$, $b$ 没有外链</p><h5 id="1-3-4-Solution-to-Spider-Traps"><a href="#1-3-4-Solution-to-Spider-Traps" class="headerlink" title="1.3.4 Solution to Spider Traps"></a>1.3.4 Solution to Spider Traps</h5><ul><li>每个时间步，随机浏览者有两个选项：<ul><li>以 $\beta$ 的概率，随机访问一个链接</li><li>以 $1-\beta$ 的概率，跳转到随机页面</li><li>$\beta \in [0.8, 0.9]$</li></ul></li><li>浏览者会在几步之中传送出去</li></ul><h5 id="1-3-5-Solution-to-Dead-Ends"><a href="#1-3-5-Solution-to-Dead-Ends" class="headerlink" title="1.3.5 Solution to Dead Ends"></a>1.3.5 Solution to Dead Ends</h5><p>如果矩阵某列的和为0，则一定是Dead Ends，则修改矩阵，让他以相等概率选择任一链接。</p><h5 id="1-3-6-Random-Teleports"><a href="#1-3-6-Random-Teleports" class="headerlink" title="1.3.6 Random Teleports"></a>1.3.6 Random Teleports</h5><blockquote><p>Google’s solution:</p><p>With prob. $\beta$, follow a link at random</p><p>with prob. $1-\beta$, jump to some random page</p></blockquote><p><strong>PageRank Equation</strong></p><script type="math/tex; mode=display">r_j=\sum_{i\rightarrow j}\beta\frac{r_i}{d_i}+(1-\beta)\frac{1}{N}</script><p>这个公式假设 $M$ 没有 dead ends，可以预处理或使用上文方法进行处理。</p><p>写成矩阵形式：</p><script type="math/tex; mode=display">G=\beta M +(1-\beta)\left[\frac{1}{N}\right]_{N\times N},r=G\cdot r</script><p>$\beta\in[0.8,0.9]$</p><hr><h3 id="2-Random-Walk-with-Restarts-and-Personalized-PageRank"><a href="#2-Random-Walk-with-Restarts-and-Personalized-PageRank" class="headerlink" title="2. Random Walk with Restarts and Personalized PageRank"></a>2. Random Walk with Restarts and Personalized PageRank</h3><p><strong>Bipartite User-Item Graph</strong></p><p>What items should we recommend to a user who interacts with item Q?</p><p><strong>intuition: </strong>if items Q and P are interacted by similar users, recommend P when user interacts with Q.</p><p>怎么比较多种上述关系的强弱？</p><p><strong>PageRank:</strong></p><p>提供图上结点的重要性并对其进行排名，同时一个随机的冲浪者在图中的任何结点上均匀传送。</p><p><strong>Personalized PageRank:</strong></p><p>每当传送时，只能跳回结点子集 $S$ 。</p><p>如果 $S$ 是单个结点怎么办？</p><p><strong>Random Walk with Restarts:</strong></p><p>Teleport back to the starting node: $S=\left\{Q\right\}$</p><p>这样的相似性考虑了多种因素：</p><ul><li>Multiple connection</li><li>Mutilple paths</li><li>Direct and indirect connections</li><li>Degree of the node</li></ul><hr><h3 id="3-结点嵌入与矩阵分解"><a href="#3-结点嵌入与矩阵分解" class="headerlink" title="3. 结点嵌入与矩阵分解"></a>3. 结点嵌入与矩阵分解</h3><p>之前通过$z_v^Tz_u$考虑结点对 $(u,v)$ 的相似性</p><p>这里我们考虑最简单的结点相似度：$u,v$ 相似如果他们相连</p><p>那么 $\boldsymbol{z_v^Tz_u}=A_{u,v}$</p><p>即</p><script type="math/tex; mode=display">Z^TZ=A</script><p>同时，嵌入的维度 $d$ （$Z$ 的行数）显然比结点数 $n$ 要小很多</p><p>所以 $A= Z^TZ$ 的精确解往往难以找到</p><p>我们可以优化以下目标：</p><script type="math/tex; mode=display">\min_{Z}||A-Z^TZ||_2</script><ul><li>优化 $Z$ 以最小化L2范数</li><li>之前使用的是softmax函数而不是L2范数，但利用 $Z^TZ$ 来逼近 $A$ 的目标是相同的</li><li>结论：通过边连通性定义的，与结点相似度有关的内积解码器等价于 $A$ 的矩阵分解</li></ul><hr><p>DeepWalk 等价于以下复杂矩阵分解</p><script type="math/tex; mode=display">log (vol(G)(\frac{1}{T}\sum_{r=1}^{T}(D^{-1}A)^r)D^{-1})-\log b</script><p>$D$ 对角阵，$D_{u,u}=\deg(u)$</p><p>$r$ 是正则邻接矩阵的幂</p><p>$T$ 是上下文窗口长度，即随机游走的长度</p><p>$vol(G)$ $G$的体积，$vol(G)=\sum_i\sum_jA_{i,j}$</p><p>$\log b$ 对于使用的负样本数量</p><p><em>Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec, WSDM 18</em></p><p><strong>Limitation</strong></p><ul><li>无法为新加入图的结点完成嵌入</li><li>无法捕获结构相似性</li><li>无法整合结点、边和图特征</li></ul><p><strong>Solution:</strong></p><p><strong>Deep Representation Learning and Graph Neural Networks</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cs224w-02</title>
      <link href="posts/1379604930/"/>
      <url>posts/1379604930/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Node-Embeddings"><a href="#1-Node-Embeddings" class="headerlink" title="1. Node Embeddings"></a>1. Node Embeddings</h3><p>Graph Representation learning alleviates the need to do feature engineering every single time.</p><p><strong>Goal:</strong>使用图进行机器学习的高效任务独立特征学习</p><p>对于每个结点，我们想学习如何在$d$维中映射这个结点，将这个$d$个数字组成的向量称为特征表示或者称为嵌入（Feature representation, embedding）</p><p><strong>Task:</strong>将结点映射到嵌入空间中</p><ul><li>结点嵌入的相似性表示它们在网络中的相似性</li><li>编码网络信息</li><li>用于不同下游预测任务</li></ul><h4 id="1-1-Encoder-and-Decoder"><a href="#1-1-Encoder-and-Decoder" class="headerlink" title="1.1 Encoder and Decoder"></a>1.1 Encoder and Decoder</h4><ul><li>$V$ is the vertex set</li><li>$A$ is the adjacency matrix</li><li>For simplicity: no node features or extra information is used</li><li><strong>Goal</strong> is to encode nodes so that similarity in the embedding space (e.g. dot product) approximates similarity in the graph</li></ul><script type="math/tex; mode=display">similarity(u,v) \approx \boldsymbol{Z_v^TZ_u}</script><ul><li>编码器（Encoder）将结点映射到嵌入</li><li>定义一个结点相似度函数（原网络相似度的一种度量）</li><li>解码器（Decoder）将嵌入映射到相似度得分，甚至可以包含点积这种</li><li>优化编码器参数满足上面的Goal</li></ul><p>最简单的Encoding方式：Encoder只是一个嵌入查找</p><script type="math/tex; mode=display">ENC(v)=\boldsymbol{z_v}=\boldsymbol{Z}\cdot v</script><p>$\boldsymbol{Z}\in \mathbb{R}^{d \times |V|}$：矩阵的每一列都是一个嵌入，也是我们学习或优化的。</p><p>$v \in \mathbb{I}^{|V|}$：全0指示向量，除指示结点$v$的那列为1</p><p>但这样的参数实在太多，我们方法的目标是直接优化每个结点的嵌入。</p><ul><li><strong>DeepWalk</strong></li><li><strong>node2vec</strong></li></ul><p><em>注：随机游走是一种无监督学习结点嵌入的方式，不使用结点标签与特征，这里的目标是直接估计结点的一组坐标（嵌入），所以网格结构的某些方面（通过DEC捕捉）得以保留</em></p><h4 id="1-2-Random-Walk-Approaches-for-Node-Embeddings"><a href="#1-2-Random-Walk-Approaches-for-Node-Embeddings" class="headerlink" title="1.2 Random Walk Approaches for Node Embeddings"></a>1.2 Random Walk Approaches for Node Embeddings</h4><ul><li><strong>Goal</strong>: $u$的嵌入$z_u$</li><li><strong>Probability</strong>: 给定结点$u$的相似程度的先验概率，用随机游走来定义，从结点$u$开始的随机游走访问结点$v$的先验概率。</li><li><strong>Softmax function</strong> and <strong>Sigmoid function</strong></li><li>Random Walk</li></ul><script type="math/tex; mode=display">\boldsymbol{Z_u^TZ_v}\approx \mathrm{probability\ that}\ u\ \mathrm{and}\ v\  \mathrm{cooccur}\ on \\\ a \ random\ walk\ over\ the\ graph</script><ol><li>从结点$u$开始使用随机游走策略$R$，估计访问结点$v$概率$P_R(v|u)$</li><li>优化嵌入去编码随机游走 $\cos \theta \propto P_R(v|u)$</li></ol><blockquote><p><strong>Why Random Walks？</strong></p><ol><li><strong>Expressivity：</strong>提供了一个灵活的结点相似性随机定义，结合局部和高阶邻居的信息。<strong>如果从结点$u$开始以高概率访问$v$，则$u$和$v$很可能相似。</strong></li><li><strong>Efficiency：</strong>不需要在训练是考虑所有结点对，只需要考虑在随机游走中共同出现是对。</li></ol></blockquote><p>Given $G=(V,E)$</p><p><strong>Goal：</strong>学习一个映射$f:u\rightarrow \mathbb{R}^d:f(u)=\boldsymbol{z}_u$</p><p>对数似然函数：</p><script type="math/tex; mode=display">\max_f \sum_{u \in V}\log P(N_R(u)|\boldsymbol{z_u})</script><p>$N_R(u)$是结点$u$使用策略$R$得到的邻居</p><hr><h5 id="1-2-1-Random-Walk-Optimization"><a href="#1-2-1-Random-Walk-Optimization" class="headerlink" title="1.2.1 Random Walk Optimization"></a>1.2.1 Random Walk Optimization</h5><ol><li>从结点$u$运行短固定长度随机游走，使用的策略为$R$</li><li>收集$N_R(u)$，使用多重集收集随机游走访问的结点</li><li>优化嵌入：给定结点$u$，预测邻居$N_R(u)$</li></ol><script type="math/tex; mode=display">\max_f \sum_{u \in V}\log P(N_R(u)|\boldsymbol{z_u})</script><p>等价地</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{u \in V}\sum_{v\in N_R(u)}-\log(P(v|\boldsymbol{z_u}))</script><p>使用softmax参数化$P(v|\boldsymbol{z_u})$</p><script type="math/tex; mode=display">P(v|\boldsymbol{z_u})=\frac{\exp(\boldsymbol{z_u^Tz_n})}{\sum_{n\in V}\exp(\boldsymbol{z_u^Tz_n})}</script><p><em>注：想更多分配概率质量给分子</em></p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-13_17-10-33.png" alt=""></p><p>优化随机游走=最小化$\mathcal{L}$</p><p>但上面两个求和给出复杂度$O(|V|^2)$</p><p>负采样：</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-13_17-10-44.png" alt=""></p><p>$\sigma$为 sigmoid 函数，$n_i$的选取以带偏移的随机方式采样。</p><p><strong>不针对网络中所有结点进行标准化，针对$k$个随机负样本$n_i$归一化</strong></p><p>如何选择负样本？</p><ul><li>采样$k$个负结点，每个具有与其度数成正比的概率</li></ul><p>$k$的选取？</p><ul><li>较高的$k$值会给更可靠的估计</li><li>较高的$k$值会导致更多采样，对负面事件有更高的偏差</li><li>实际上，$k$=5-20</li></ul><p>梯度下降法：</p><ul><li>初始化结点的嵌入$z_i$</li><li>迭代直到收敛<ul><li>对所有$i$，计算$\frac{\partial \mathcal{L}}{\partial z_i}$</li><li>$z_i\leftarrow z_i-\eta \frac{\partial \mathcal{L}}{\partial z_i}$</li></ul></li></ul><p>随机梯度下降：</p><p>以随机方式逼近梯度，不对所有的进行评估，而选择其中的一小部分</p><ul><li>初始化结点的嵌入$z_i$</li><li>迭代直到收敛：$\mathcal{L}^{(u)}=\sum_{v\in N_R(u)}-\log(P(v|\boldsymbol{z_u}))$<ul><li>对一个结点$i$，对所有的$j$计算$\frac{\partial \mathcal{L}^{(i)}}{\partial z_j}$</li><li>$z_j\leftarrow z_j-\eta \frac{\partial \mathcal{L}^{(i)}}{\partial z_j}$</li></ul></li></ul><hr><h5 id="1-2-2-总结"><a href="#1-2-2-总结" class="headerlink" title="1.2.2 总结"></a>1.2.2 总结</h5><ul><li>从图上每个结点开始的固定长度的短随机游走</li><li>对于每个$u$，收集多重集$N_R(u)$</li><li>使用随机梯度优化$\mathcal{L}$，可以使用负采样进行有效近似</li></ul><p><strong>How should we randomly walk？</strong></p><p>最简单的想法：从每个结点开始运行固定长度无偏随机游走</p><p>但这可能太过限制了，推广到 node2vec</p><hr><h4 id="1-3-node2vec"><a href="#1-3-node2vec" class="headerlink" title="1.3 node2vec"></a>1.3 node2vec</h4><p><strong>Goal：</strong>在特征空间中紧密嵌入具有相似网络邻域的节点</p><p><strong>Key observation：</strong>灵活的网络邻域概念会导致更丰富结点嵌入</p><p>构造偏移2阶随机游走为$u$生成邻域$N_R(u)$</p><p><strong>Idea：</strong>使用灵活的有偏随机游走，在局部视图与全局视图之间进行权衡</p><p>来自 DFS 和 BFS 的直觉</p><p>Biased fixed-length random walk $R$ that given a node $u$ generates neighborhood $N_R(u)$</p><p>两个超参数：</p><ul><li><p>Return parameter $p$</p><ul><li>回到上一个结点的概率</li></ul></li><li><p>In-out parameter $q$</p><ul><li>Moving outwards (DFS) vs. moving inwards (BFS)</li><li>Intuitively, $q$ is the ratio of BFS vs. DFS</li></ul></li></ul><p><strong>Biased $2^{nd}$-order random walks explore network neighborhoods</strong></p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-12_23-25-21.png" alt=""></p><p><strong>算法：</strong></p><ul><li>计算随机游走概率</li><li>模拟 $r$  有偏随机游走，长度为 $l$</li><li>使用随机梯度下降优化</li></ul><p>线性时间复杂度，三个步骤可以并行。</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-12_23-31-26.png" alt=""></p><h3 id="2-Embedding-Entire-Graphs"><a href="#2-Embedding-Entire-Graphs" class="headerlink" title="2. Embedding Entire Graphs"></a>2. Embedding Entire Graphs</h3><p><strong>Goal:</strong>embed a subgraph or an entire graph $G$ to $\boldsymbol{z_G}$</p><p><strong>Tasks:</strong> 通过分子分类来预测哪些分子有毒，哪些无毒；图形异常检测</p><h4 id="2-1-Approach-1"><a href="#2-1-Approach-1" class="headerlink" title="2.1 Approach 1"></a>2.1 Approach 1</h4><ul><li>运行标准的结点嵌入</li><li>对所有的嵌入求和或求平均</li></ul><script type="math/tex; mode=display">\boldsymbol{z_G=\sum_{v\in G}z_v}</script><h4 id="2-2-Approach-2"><a href="#2-2-Approach-2" class="headerlink" title="2.2 Approach 2"></a>2.2 Approach 2</h4><p>引入一个虚拟结点来表示整个图或子图，然后运行标注的图嵌入或结点嵌入技术。</p><p>虚拟结点与图或子图中的每个结点相连。</p><h4 id="2-3-Anonymous-Walk-Embeddings"><a href="#2-3-Anonymous-Walk-Embeddings" class="headerlink" title="2.3 Anonymous Walk Embeddings"></a>2.3 Anonymous Walk Embeddings</h4><p>匿名游走不将随机游走表示为它访问的结点序列，而是第一次访问结点的时间序列</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-13_14-32-49.png" alt=""></p><p>随着匿名游走的长度增加，其数目呈指数上升。</p><h5 id="2-3-1-Simple-Use"><a href="#2-3-1-Simple-Use" class="headerlink" title="2.3.1 Simple Use"></a>2.3.1 Simple Use</h5><p>模拟长度为 $l$ 的匿名游走，并记录它们的计数，将图表示为这些游走的概率分布</p><p><strong>Example:</strong></p><ul><li>$l$ = 3</li><li>可以将图表示为 5 维向量，因为有 5 种长度为 3 的匿名游走 $w_i$：111，112，121，122，123</li><li>$Z_G[i]$ 为 $G$ 在匿名游走 $w_i$ 中的概率</li></ul><p><strong>采样匿名游走：</strong>生成独立的 $m$ 个随机游走，再将图表示为这些游走的概率分布</p><p>distribution have error of no more than $\varepsilon$ with prob. less than $\delta$:</p><script type="math/tex; mode=display">m=\lceil\frac{2}{\varepsilon}(\log (2^\eta-2)-\log(\delta))\rceil</script><p> $\eta$ 为长度 $l$ 的匿名游走的种数</p><h5 id="2-3-2-Learn-Walk-Embeddings"><a href="#2-3-2-Learn-Walk-Embeddings" class="headerlink" title="2.3.2 Learn Walk Embeddings"></a>2.3.2 Learn Walk Embeddings</h5><p>学习匿名游走 $w_i$ 的嵌入 $z_i$，将 $Z_G$ 与所有匿名游走 $z_i$ 一起学习</p><p><strong>如何学习？</strong></p><p>要学习的 $z_G$ 为整个图的嵌入</p><p>从结点1出发，使用采样匿名游走</p><p>想法是学习预测在 $\Delta$ 窗口大小下共同发生的游走</p><script type="math/tex; mode=display">\max \sum_{t=\Delta}^{T-\Delta} \log P(w_t|w_{t-\Delta},\dots,w_{t+\Delta},z_G)</script><ul><li>从 $u$ 运行 $T$ 个不同的长度为 $l$ 的随机游走</li></ul><script type="math/tex; mode=display">N_R(u)=\left\{w_1^u,\dots w_T^u\right\}</script><ul><li>学习预测同时出现在 $\Delta$ 大小窗口的游走</li><li>使用匿名游走 $w_i$ 来估计嵌入 $z_i$</li></ul><script type="math/tex; mode=display">\max_{Z,d} \frac{1}{T}\sum_{t=\Delta}^{T-\Delta}\log P(w_t|\left\{w_{t-\Delta},\dots,w_{t+\Delta},z_G\right\})</script><p><strong>$\boldsymbol{z_G}$的使用</strong>：图分类</p><ul><li>点积：$z_{G1}^Tz_{G2}$</li><li>作为神经网络的输入</li></ul><hr><h3 id="3-Summary：How-to-use-Embeddings"><a href="#3-Summary：How-to-use-Embeddings" class="headerlink" title="3. Summary：How to use Embeddings"></a>3. Summary：How to use Embeddings</h3><ul><li><strong>Clustering/community detection:</strong> Cluster points $z_i$</li><li><strong>Node classification:</strong> Predict label of node $i$ based on $z_i$</li><li><strong>Link prediction:</strong> Predict edge $(i,j)$ based on $(z_i,z_j)$<ul><li>Concatenate: $f(z_i,z_j)=g([z_i,z_j])$</li><li>Hadamard: $f(z_i,z_j)=g(z_i * z_j)$</li><li>Sum/Avg: $f(z_i,z_j)=g(z_i+z_j)$</li><li>Distance: $f(z_i,z_j)=g(||z_i-z_j||_2)$</li></ul></li><li><strong>Graph classification:</strong> graph embedding $z_G$ via aggregating node embeddings or anonymous random walks. </li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224W-01</title>
      <link href="posts/277284787/"/>
      <url>posts/277284787/</url>
      
        <content type="html"><![CDATA[<h3 id="1-图机器学习的应用："><a href="#1-图机器学习的应用：" class="headerlink" title="1. 图机器学习的应用："></a>1. 图机器学习的应用：</h3><h4 id="1-1-Node-level"><a href="#1-1-Node-level" class="headerlink" title="1.1 Node-level"></a>1.1 Node-level</h4><ul><li>蛋白质折叠$\rightarrow$AlphaFold</li></ul><h4 id="1-2-Edge-level"><a href="#1-2-Edge-level" class="headerlink" title="1.2 Edge-level"></a>1.2 Edge-level</h4><ul><li>推荐系统</li><li>药物副作用</li></ul><h4 id="1-3-Subgraph-level"><a href="#1-3-Subgraph-level" class="headerlink" title="1.3 Subgraph-level"></a>1.3 Subgraph-level</h4><ul><li>交通预测</li></ul><h4 id="1-4-Graph-level"><a href="#1-4-Graph-level" class="headerlink" title="1.4 Graph-level"></a>1.4 Graph-level</h4><ul><li>药物发现</li><li>分子生成、优化</li><li>物理模拟</li></ul><h3 id="2-图形表示的选择"><a href="#2-图形表示的选择" class="headerlink" title="2. 图形表示的选择"></a>2. 图形表示的选择</h3><ul><li>边、结点</li><li>无向图 vs. 有向图</li><li>结点度数 vs. 出度入度</li><li>二部图 $\rightarrow$ 折叠网络（只使用二部图一侧的结点，如果两个结点至少有一个共同的邻居，则在其之间创建连接，生成两个图）</li><li>邻接矩阵</li><li>稀疏</li><li>边列表  vs. 邻接表</li><li>附加属性：<ul><li>权重</li><li>排名</li><li>类型</li><li>倾向</li></ul></li><li>连通性 vs. 强连通&amp;弱连通</li><li>强连通分量</li></ul><h3 id="3-传统方法"><a href="#3-传统方法" class="headerlink" title="3. 传统方法"></a>3. 传统方法</h3><h4 id="3-1-步骤："><a href="#3-1-步骤：" class="headerlink" title="3.1 步骤："></a>3.1 步骤：</h4><ul><li>获取数据点、结点、链接、图，训练经典机器学习模型</li><li>将这个模型应用于新结点并作出预测</li></ul><h4 id="3-2-特征设计（手工设计特征）"><a href="#3-2-特征设计（手工设计特征）" class="headerlink" title="3.2 特征设计（手工设计特征）"></a>3.2 特征设计（手工设计特征）</h4><ul><li>Node-level</li><li>Link-level</li><li>Graph-level</li></ul><p>着重关注无向图，目标：对一组感兴趣的对象进行预测。</p><p><strong>Given:$G=(V,E)$</strong></p><p><strong>Learn a function:$f:V\rightarrow \mathbb{R}$</strong></p><h4 id="3-3-Node-Level"><a href="#3-3-Node-Level" class="headerlink" title="3.3 Node-Level"></a>3.3 Node-Level</h4><blockquote><p>表述一个结点在网络中的结构与位置</p></blockquote><hr><h5 id="3-3-1-Node-degree"><a href="#3-3-1-Node-degree" class="headerlink" title="3.3.1 Node degree"></a>3.3.1 Node degree</h5><p><strong>Treat all neighboring nodes equally</strong></p><p>度数相同可能无法区分</p><hr><h5 id="3-3-2-Node-centrality（结点中心性度量）"><a href="#3-3-2-Node-centrality（结点中心性度量）" class="headerlink" title="3.3.2 Node centrality（结点中心性度量）"></a>3.3.2 Node centrality（结点中心性度量）</h5><p>试图描述或捕捉图中结点重要性这一概念。$c_v$</p><ul><li><strong>Eigenvector centrality</strong></li></ul><p>A node $v$ is important if surrounded by important neiboring nodes $u \in N(v)$. </p><script type="math/tex; mode=display">c_v=\frac{1}{\lambda}\sum_{u\in N(v)}c_u \Longleftrightarrow\lambda \boldsymbol{c=Ac}</script><p>$\boldsymbol{A}$邻接矩阵，$\boldsymbol{c}$中心性度量的向量，可以得到：</p><p><strong>$\lambda_{max}$ is always positive and unique,the leading eigenvector $c_{max}$ is used for centrality.</strong></p><ul><li><strong>Betweenness centrality</strong></li></ul><p>A node is important if it lies on many shortest paths between other nodes.</p><script type="math/tex; mode=display">c_v=\sum_{s\neq v\neq t}\frac{\#(\mathrm{shortest\  paths\  between}\  s\  \mathrm{and}\  t\  \mathrm{that\  contain}\  v)}{\# (\mathrm{shortest\ paths \ between}\ s\ \mathrm{and}\ t)}</script><ul><li><strong>Closeness centrality</strong></li></ul><p>A node is important if it has small shortest path lengths to all other nodes.</p><script type="math/tex; mode=display">c_v=\frac{1}{\sum_{u\neq v}\mathrm{shortest\ path \ length\ between }\ u\ \mathrm{and}\ v}</script><hr><h5 id="3-3-3-Clustering-coefficient"><a href="#3-3-3-Clustering-coefficient" class="headerlink" title="3.3.3 Clustering coefficient"></a>3.3.3 Clustering coefficient</h5><p>Measures how connected $v’$s neighboring nodes are:</p><script type="math/tex; mode=display">e_v=\frac{\#(\mathrm{edges\ among \ neighboring\ nodes})}{\left(\begin{array}{c}k_v\\2\end{array}\right)}\in \boldsymbol{[0,1]}</script><p>$k_v$是$v$的度数，注意分子为$v$的邻居们之间连了几条边。</p><h5 id="3-3-4-Graphlets"><a href="#3-3-4-Graphlets" class="headerlink" title="3.3.4 Graphlets"></a>3.3.4 Graphlets</h5><p>Clustering coefficient counts the #(triangles) in the ego-network</p><p>ego-network是由结点本身及其邻居诱导的网络。</p><p><strong>We can generalize the above by counting #(pre-spcified subgraphs i.e. graphlets)</strong></p><p><strong>graphlets</strong>: Rooted connected non-isomorphic subgraphs</p><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/Snipaste_2021-06-10_23-31-05.png" alt=""></p><p><strong>Graphlet Degree Vector(GDV)</strong>:Graphlet-base features for nodes</p><ul><li>Degree 统计一个点接触的边数</li><li>Clustering coefficient 统计一个点接触的三角形数</li><li>GDV 统计一个点接触的graphlets的个数</li></ul><p>GDV 提供了一个结点的局部网络拓扑的度量，我们可以用73维的向量描述每一个结点</p><hr><h5 id="3-3-5-总结"><a href="#3-3-5-总结" class="headerlink" title="3.3.5 总结"></a>3.3.5 总结</h5><p>分类：</p><ul><li>基于重要性的特征，用于预测图中结点的重要性：识别社交网络中的名人用户<ul><li>Node degree</li><li>Node centrality measures</li></ul></li><li>基于结构的特征，用于预测特定的结点角色：预测蛋白质功能<ul><li>Node degree</li><li>Clustering coefficient</li><li>Graphlet Degree Vector</li></ul></li></ul><h4 id="3-4-Link-Level"><a href="#3-4-Link-Level" class="headerlink" title="3.4 Link-Level"></a>3.4 Link-Level</h4><p>任务是根据网络中现有的链接预测新的链接，所以要评估所有尚未链接的结点对，给出一个排名，前K个为预测的结果。关键就是为一对结点设计特征。</p><p>两种预测问题的表述：</p><ul><li>链接随机丢失（静态网络）：随机移除一些链接，然后尝试用机器学习算法将它们预测回来</li><li>随着时间推移预测链接（随时间发展的网络）：给定直到时间$t_0’$的图$G[t_0,t_0’]$，输出一个排名列表$L$，包含预测会出现在$G[t_1,t_1’]$但不在$G[t_0,t_0’]$中的链接。<ul><li>$n=|E_{new}|$: <script type="math/tex">\#\  \mathrm{new\ edges\ that\ appear\ during\ the\ test\ period\ } [t_1,t_1']</script></li><li>$\mathrm{Take\ top\ }n\ \mathrm{elements\ of\ }L\ \mathrm{and\ count\ correct\ edges}$</li></ul></li></ul><blockquote><p><strong>方法论</strong></p><ul><li>对于结点对$(x,y)$，计算分数$c(x,y)$</li><li>依据$c(x,y)$进行降序排列</li><li>预测最高的$n$个结点对作为新链接</li><li>观察哪些链接是实际上出现在$G[t_1,t_1’]$中的</li></ul></blockquote><h5 id="3-4-1-Distance-based-feature"><a href="#3-4-1-Distance-based-feature" class="headerlink" title="3.4.1 Distance-based feature"></a>3.4.1 Distance-based feature</h5><p>两个结点的最短路径距离，显然这种特征没有捕捉邻域重叠程度或链接强度</p><h5 id="3-4-2-Local-neighborhood-overlap"><a href="#3-4-2-Local-neighborhood-overlap" class="headerlink" title="3.4.2 Local neighborhood overlap"></a>3.4.2 Local neighborhood overlap</h5><p><strong>Captures # neighboring nodes shared between two nodes $v1$ and $v2$</strong></p><ul><li>Common neighbors:$|N(v_1)\cap N(v_2)|$</li><li>Jaccard’s coefficient:$\frac{|N(v_1)\cap N(v_2)|}{|N(v_1)\cup N(v_2)|}$</li><li>Adamic-Adar index:$\sum_{u\in N(v_1)\cap N(v_2)}\frac{1}{\mathrm{log}(k_u)}$</li></ul><p><strong>Limitaton:</strong>两个结点没有共同邻居时度量总为0，但它们在未来仍有可能相连</p><h5 id="3-4-3-Global-neighborhood-overlap"><a href="#3-4-3-Global-neighborhood-overlap" class="headerlink" title="3.4.3 Global neighborhood overlap"></a>3.4.3 Global neighborhood overlap</h5><p><strong>Katz index</strong>: 统计给定的结点对之间路径的所有不同长度</p><ul><li>计算两个结点之间的路径数：使用邻接矩阵的幂$A_{uv}^l$计算$u,v$结点之间长度为$l$的路径数</li></ul><p><strong>Katz index</strong>可表示为：</p><script type="math/tex; mode=display">S_{v_1v_2}=\sum_{l=1}^{\infty}\beta^lA_{v_1v_2}^l</script><p>$\beta\in(0,1)$给越长的路径设定越低的重要性</p><p>闭式计算：</p><script type="math/tex; mode=display">S=\sum_{i=1}^{\infty}\beta^iA^i=(I-\beta A)^{-1}-I</script><h4 id="3-5-Graph-Level"><a href="#3-5-Graph-Level" class="headerlink" title="3.5 Graph-Level"></a>3.5 Graph-Level</h4><p><strong>Goal</strong>:features that characterize the structure of an entire graph</p><p><strong>Kernel methods</strong>:Design kernels instead of feature vectors</p><blockquote><p>Quick introduction:</p><ul><li>Kernal $K(G,G’)\in \mathbb{R}$ measures similarity b/w data</li><li>Kernel matrix $\boldsymbol{K}=(K(G,G’))_{G,G’}$must be positive semidefinite</li><li>There exists a feature representation $\phi(\cdot)$ such that $K(G,G’)=\phi(G)^T\phi(G’)$</li><li>Once the kernel is defined,off-the-shelf ML model such as kernel SVM can be used to make predictions</li></ul></blockquote><p>举例：</p><ul><li>Graphlet Kernel</li><li>Weisfeiler-Lehman Kernel</li><li>Random-walk Kernel</li><li>Shortest-path graph Kernel</li></ul><h5 id="3-5-1-Goal-Design-graph-feature-vector-boldsymbol-phi-G"><a href="#3-5-1-Goal-Design-graph-feature-vector-boldsymbol-phi-G" class="headerlink" title="3.5.1 Goal:Design graph feature vector $\boldsymbol{\phi(G)}$"></a>3.5.1 Goal:Design graph feature vector $\boldsymbol{\phi(G)}$</h5><p><strong>Key idea:</strong>Bags-of-Words for a graph$\color{Red}{(图的词袋表示)}$</p><p>首先不能简单将结点视为单词，相同结点数目输出相同。</p><p>使用 Bags of node degrees 就能为不同的图生成不同特征。</p><p><strong>Graphlet Kernel</strong>与<strong>Weisfeiler-Lehman(WL) Kernel</strong> 都采用这种Bag-of-*表示方法，*是比结点度数更为复杂的特征。</p><h5 id="3-5-2-Graphlet-Features"><a href="#3-5-2-Graphlet-Features" class="headerlink" title="3.5.2 Graphlet Features"></a>3.5.2 Graphlet Features</h5><p><strong>Key idea:</strong>Count the number of different graphlets in a graph.</p><p><em>注：这里定义的 graphlets 与 node-level 中的有所不同</em></p><ul><li>Nodes in graphlets here do not need to be connected</li><li>The graphlets here are not rooted</li></ul><p>Given graph $G$, a graphlet list $\mathcal{G_k}=(g_1,\dots,g_{n_k})$,define the graphlet count vector $f_G\in \mathbb{R}^{n_k}$ as</p><script type="math/tex; mode=display">(f_G)_i= \#(g_i \subseteq G) \ for \ i= 1,\dots n_k</script><script type="math/tex; mode=display">K(G,G')=\boldsymbol{f_G^Tf_{G^{\boldsymbol{'}}}}</script><p><strong>Problem:</strong>$G$ 和 $G^{‘}$大小不同时，会引起值的偏向。</p><p><strong>Solution:</strong>对特征向量归一化</p><script type="math/tex; mode=display">\boldsymbol{h_G=\frac{f_G}{Sum(f_G)}}</script><script type="math/tex; mode=display">K(G,G')=\boldsymbol{h_G^Th_{G^{\boldsymbol{'}}}}</script><p><strong>Limitations:</strong>统计graphlets非常昂贵</p><h5 id="3-5-3-WL-Kernel"><a href="#3-5-3-WL-Kernel" class="headerlink" title="3.5.3 WL Kernel"></a>3.5.3 WL Kernel</h5><p><strong>Goal:</strong>设计更加有效的办法</p><p><strong>Idea:</strong>使用邻域结构迭代地丰富结点词典，推广Bag of node degrees，从一跳邻域信息到多跳邻域信息。</p><p><strong>Algorithm:</strong>Color refinement（颜色细化）或 Weisfeiler-Lehman图同构检验</p><p><strong>Given:</strong>带有一组结点$V$，为每个结点$v$分配初始颜色$c^{(0)}(v)$，迭代</p><script type="math/tex; mode=display">c^{(k+1)}(v)=\boldsymbol{HASH}(\left\{c^{(k)}(v),\left\{c^{(k)}(u)\right\}_{u \in N(v)}\right\})</script><p><strong>HASH</strong>将不同输入映射到不同颜色</p><p>经过$K$步，得到的$c^{(K)}(v)$总结了K跳邻域的结构。</p><p>在颜色细分后，WL Kernel统计给定颜色的结点数，得到一个向量</p><p>$K(G,G’)$仍然定义为向量的点积。</p><p>时间复杂度关于边数是线性的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN综述</title>
      <link href="posts/3892482635/"/>
      <url>posts/3892482635/</url>
      
        <content type="html"><![CDATA[<h3 id="Spectral-methods-amp-Spatial-methods"><a href="#Spectral-methods-amp-Spatial-methods" class="headerlink" title="Spectral methods &amp; Spatial methods"></a>Spectral methods &amp; Spatial methods</h3><ul><li>谱方法在谱域里定义 ，通过Fourier变化到谱域，在谱域实现卷积定义;</li><li>空间方法在结点域定义卷积，由于结点邻域大小不同，实现参数共享困难，关键就是参数共享。</li></ul><h2 id="1-Spectral-methods"><a href="#1-Spectral-methods" class="headerlink" title="1. Spectral methods"></a>1. Spectral methods</h2><h4 id="1-1-1-输入"><a href="#1-1-1-输入" class="headerlink" title="1.1.1 输入"></a>1.1.1 输入</h4><p>图$\boldsymbol{G = (V,E,W)}$</p><p>$\boldsymbol{V}$为点集，$\boldsymbol{n=|V|}$，$\boldsymbol{E}$为边集，$\boldsymbol{W \in R^{n\times n}}$权重集合。</p><p>每一个点都有一个$\boldsymbol{d}$维特征，所以有$\boldsymbol{X \in  R^{n\times d}}$的特征矩阵，每一列可以看做定义在这些结点上的信号。</p><h4 id="1-1-2-图上的Laplacian"><a href="#1-1-2-图上的Laplacian" class="headerlink" title="1.1.2 图上的Laplacian"></a>1.1.2 图上的Laplacian</h4><p>对图做谱分析的基本工具，定义了图上的导数，刻画了信号的平滑程度。</p><ul><li>$\boldsymbol{L=D-W}$</li><li>归一化版本 $\boldsymbol{L_{norm}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}=I-D^{-\frac{1}{2}}WD^{-\frac{1}{2}}}$，将主对角线全变为1</li><li>$L$为实对称矩阵，可以用正交矩阵进行正交相似对角化。</li></ul><script type="math/tex; mode=display">L=U\Lambda U^T</script><ul><li>以$L$为系数矩阵的二次型，图的总变差：</li></ul><script type="math/tex; mode=display">x^TLx=x^TU\Lambda U^Tx=\sum_{i=1}^{n}\sum_{j\in N(i)}x_i(x_i-x_j)=\sum_{e_{ij}}(x_i-x_j)^2</script><ul><li>$L$具有以下性质：<ul><li>每行元素之和为零，至少有一个特征值为0，对应特征向量$u_0=\frac{[1,1,\dots1]^T}{\sqrt{n}}$,$\textbf{L}u_0=0u_0$</li><li>特征值都大于等于0，归一化拉普拉斯矩阵特征值区间为[0,2]</li></ul></li></ul><p>图微分的定义：</p><ul><li>第$i$个结点处，一阶偏导</li></ul><script type="math/tex; mode=display">\frac{\partial \boldsymbol{x}}{\partial e(i,j)}=\sqrt{W_{i,j}}(x_j-x_i)</script><ul><li>第$i$个结点处，梯度是一个向量</li></ul><script type="math/tex; mode=display">\nabla_i \boldsymbol{x}=\left\{\frac{\partial \boldsymbol{x}}{\partial e(i,j)}|j\in N(i)\right\}</script><ul><li>图的局部平滑度</li></ul><script type="math/tex; mode=display">||\nabla_i \boldsymbol{x}||=\left[{\sum_{j\in N(i)}\left(\frac{\partial \boldsymbol{x}}{\partial e(i,j)}\right)^2}\right]^{\frac{1}{2}}=\left[{\sum_{j\in N(i)}W_{ij}\left(x_j-x_i\right)^2}\right]^{\frac{1}{2}}</script><ul><li>全局平滑度：离散p-狄利克雷型</li></ul><script type="math/tex; mode=display">S_p(x)=\frac{1}{p}\sum_{i}^{V}||\nabla_i \boldsymbol{x}||_2^p</script><ul><li>$p=2$时，</li></ul><script type="math/tex; mode=display">S_2(x)=\frac{1}{2}\sum_{i\in V}\sum_{j\in N(i)}W_{i,j}(x_j-x_i)^2=\sum_{e(i,j)\in E}W_{i,j}(x_j-x_i)^2</script><h4 id="1-1-3-Fourier基"><a href="#1-1-3-Fourier基" class="headerlink" title="1.1.3 Fourier基"></a>1.1.3 Fourier基</h4><p>$\boldsymbol{n}$个结点，每个结点有一个取值，图上的一个信号就是$\boldsymbol{n}$维向量，要变换到新的域里，就需要一组基，这就是$\boldsymbol{L}$的$\boldsymbol{n}$个特征向量，彼此正交。取基为特征向量$\left\{ u_l \right\}_{l=1}^n$，由非负特征值$\left\{ \lambda_l \right\}_{l=1}^n$排序 。可以证明更小的特征值对应的特征向量更平滑，因此我们可以将拉普拉斯矩阵的特征值等价成频率。低频特征向量变化缓慢，其直观的效果是两个被大权重边相连的节点它们的信号值十分接近；高频特征向量则剧烈振荡，相邻节点的信号值差异较大。</p><p>$L$的正交相似对角化：</p><script type="math/tex; mode=display">L=U\Lambda U^{T}</script><p>$\boldsymbol{U=\left[u_1,\dots ,u_n\right]}$，$\boldsymbol{\Lambda=diag\left(\left[\lambda_1,\dots ,\lambda_n\right] \right)}$。</p><p>对于一个信号$x\in R^n$，Fourier变换定义为</p><script type="math/tex; mode=display">\hat{x}=U^{T}x</script><p>逆变换</p><script type="math/tex; mode=display">x=U\hat{x}</script><p>总变差</p><script type="math/tex; mode=display">x^TLx=x^TU\Lambda U^Tx=\hat x^T\Lambda\hat{x}=\sum_{i=1}^{n}\lambda_i\hat{x_i}^2</script><h4 id="1-1-4-卷积"><a href="#1-1-4-卷积" class="headerlink" title="1.1.4 卷积"></a>1.1.4 卷积</h4><p>两个信号的卷积可以看作Fourier变换后的点积。</p><script type="math/tex; mode=display">x \ast_G y =U((U^Tx)\odot(U^Ty))</script><p>这里的卷积核为$U^Ty$</p><p>令$U^Ty=[\theta_0,\dots,\theta_{n-1}]^{T}$，将其变为矩阵$g_\theta=diag([\theta_0,\dots,\theta_{n-1}])$</p><p> $g_\theta$是有n个参数的对角阵，参数是自由的、需要学习的参数</p><p>上式可变化为</p><script type="math/tex; mode=display">x \ast_G y = Ug_\theta U^Tx</script><hr><h3 id="1-2-Spectral-Graph-CNN"><a href="#1-2-Spectral-Graph-CNN" class="headerlink" title="1.2 Spectral Graph CNN"></a>1.2 Spectral Graph CNN</h3><script type="math/tex; mode=display">x_{k+1,j}=h\left(\sum_{i=1}^{f_k}UF_{k,i,j}U^Tx_{k,i}\right),j=1,\dots,f_{k+1}</script><p>$x_{k,i}$是第k层的信号，$F_{k,i,j}$是第k层的核，$h(\cdot)$为卷积运算。</p><h4 id="1-2-1-Spectral-Graph-CNN的短板"><a href="#1-2-1-Spectral-Graph-CNN的短板" class="headerlink" title="1.2.1 Spectral Graph CNN的短板"></a>1.2.1 Spectral Graph CNN的短板</h4><ul><li>依赖于Laplacian矩阵分解，非常复杂$\boldsymbol O\left(n^3\right)$</li><li>高计算代价，稠密的矩阵乘$\boldsymbol x$复杂度$\boldsymbol O\left(n^2\right)$</li><li>在结点域是非局部的(not localized)</li></ul><hr><h3 id="1-3-ChebyNet"><a href="#1-3-ChebyNet" class="headerlink" title="1.3 ChebyNet"></a>1.3 ChebyNet</h3><p>使用多项式函数对卷积核作参数化</p><p>将$g_\theta$变为</p><script type="math/tex; mode=display">g_{\beta}\left(\Lambda\right)=\sum_{k=0}^{K-1}\beta_k\Lambda^k, \Lambda=diag(\lambda_1,\dots,\lambda_n)</script><p>这样</p><script type="math/tex; mode=display">x \ast_G y = Ug_\beta(\Lambda) U^Tx=\sum_{k=0}^{K-1}\beta_kL^kx</script><ul><li>不需要特征分解</li><li>L稀疏，降低复杂度为$\boldsymbol{O(|E|)}$</li><li>L为局部化函数，卷积只受k跳邻居邻居影响</li></ul><h4 id="1-3-1-切比雪夫多项式"><a href="#1-3-1-切比雪夫多项式" class="headerlink" title="1.3.1 切比雪夫多项式"></a>1.3.1 切比雪夫多项式</h4><p>引入切比雪夫多项式近似$L^k$,$T_k(x)$为k阶切比雪夫多项式</p><script type="math/tex; mode=display">g_{\beta}\left(\Lambda\right)=\sum_{k=0}^{K-1}\beta_kT_k(\tilde\Lambda), \tilde\Lambda=\frac{2}{\lambda_{max}}\Lambda-I_N</script><p>切比雪夫多项式的输入在$[-1,1]$之间。</p><hr><h3 id="1-4-Graph-wavelet-nerual-network"><a href="#1-4-Graph-wavelet-nerual-network" class="headerlink" title="1.4 Graph wavelet nerual network"></a>1.4 Graph wavelet nerual network</h3><p>ChebyNet通过约束卷积取值空间，限制为$\Lambda$的多项式函数。</p><p>之前的问题主要来自$\boldsymbol{U}$ 的性质不够好，所以采用小波基来代替Fourier基。</p><p>小波基具有以下性质：</p><ul><li>稀疏</li><li>局部化</li><li>低计算代价</li></ul><div class="table-container"><table><thead><tr><th></th><th>$Fourier$</th><th>$Wavelet$</th></tr></thead><tbody><tr><td>$basis$</td><td>$U$</td><td>$\psi_s=Ue^{\lambda s}U^{T}$</td></tr><tr><td>$transform$</td><td>$\hat{x}=U^Tx$</td><td>$x^{\ast}=\psi_s^{-1}x$</td></tr><tr><td>$inverse \ transform$</td><td>$x=U\hat{x}$</td><td>$x=\psi_Sx^{\ast}$</td></tr><tr><td>$convolution$</td><td>$x \ast_G y =U((U^Ty)\odot(U^Tx))$</td><td>$x \ast_G y = \psi_s((\psi_s^{-1}y)\odot(\psi_s^{-1}x))$</td></tr><tr><td>$neural\ network$</td><td>$x_{k+1,j}=h\left(\sum_{i=1}^{p}UF_{k,i,j}U^Tx_{k,i}\right),\\j=1,\dots,q$</td><td>$x_{k+1,j}=h\left(\sum_{i=1}^{p}\psi_s F_{k,i,j}\psi_s ^{-1}x_{k,i}\right),\\j=1,\dots,q$</td></tr></tbody></table></div><p>复杂度：$O(n \ast p \ast q)$难以接受</p><h4 id="1-4-1-Key-idea"><a href="#1-4-1-Key-idea" class="headerlink" title="1.4.1 Key idea"></a>1.4.1 Key idea</h4><p>图卷积与图的特征变换不能同时进行，否则参数个数非常大，拆分为：</p><script type="math/tex; mode=display">x_{k+1,j}=h\left(\sum_{i=1}^{p}\psi_s F_{k,i,j}\psi_s ^{-1}x_{k,i}\right),\\j=1,\dots,q</script><script type="math/tex; mode=display">\Downarrow</script><script type="math/tex; mode=display">\boldsymbol{Feature\ transformation}\\y_{k,j}=\sum_{i=1}^pT_{ji}x_{k,i},T \in R^{q\times p}with \ p \ast q \ parameters</script><script type="math/tex; mode=display">+</script><script type="math/tex; mode=display">\boldsymbol{Graph \ convolution}\\x_{k+1,j}=h\left(\psi_sF_k\psi_s^{-1}y_{k,j}\right),F_k\  is \ a \ diagonal\ matrix \ with\  n \ parameters</script><p>复杂度：$O(n + p \ast q)$</p><p><strong>谱方法为空间方法的特例</strong></p><hr><h2 id="2-Spatial-methods"><a href="#2-Spatial-methods" class="headerlink" title="2. Spatial methods"></a>2. Spatial methods</h2><h3 id="2-1-通过类比"><a href="#2-1-通过类比" class="headerlink" title="2.1 通过类比"></a>2.1 通过类比</h3><ol><li><strong>确定邻居</strong></li><li>给邻居结点编序</li><li>参数共享</li></ol><p>对每一个结点，为它选择固定数量的邻居，定义一个邻近度的度量。</p><h3 id="2-2-GraphSAGE"><a href="#2-2-GraphSAGE" class="headerlink" title="2.2 GraphSAGE"></a>2.2 GraphSAGE</h3><p>选固定的个数，直接随机选择得了，用随机行走选固定个数。</p><ul><li><p>Sampling neighbors</p></li><li><p>Aggregating neighbors</p></li></ul><script type="math/tex; mode=display">a_v^{(k)}=AGGREGATE^{(k)} \left(\left\{h_u^{(k-1)}:u \in \mathcal{N(v)} \right\}\right)\\h_v^{(k)}=COMBINE^{(k)}\left(h_v^{(k-1)},a_v^{(k)}\right)\\\color{Red}{Aggregate\ the \ information\ of \ neighboring\ nodes\ to\ update\ the\ representation\ of\ center\ node.}</script><h3 id="2-3-GCN"><a href="#2-3-GCN" class="headerlink" title="2.3 GCN"></a>2.3 GCN</h3><p>ChebyNet的简化版本，但已经是空间方法了,进一步选取$K=2$，$\lambda_{max}=2$，。</p><ul><li>通过正则化Laplacian矩阵聚合邻域信息</li><li>由特征变化共享参数</li></ul><script type="math/tex; mode=display">g_\theta \ast x \approx \theta_0+\theta_1(L-I_N)x=\theta_0x-\theta_1D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}}x</script><p>在实际训练过程中，我们需要规范化参数来避免过拟合，所以我们令 $\theta=\theta_0^{‘}=-\theta_1^{‘}$</p><script type="math/tex; mode=display">g_\theta \ast x \approx \theta (I_N+ D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}})x</script><p>实际上$I_N+ D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}}$的特征值在$[0,2]$之间，需要再次归一化。</p><script type="math/tex; mode=display">I_N+ D^{-{\frac{1}{2}}}AD^{-{\frac{1}{2}}}\to \tilde D^{-{\frac{1}{2}}}\tilde A\tilde D^{-{\frac{1}{2}}},\tilde D_{ii}=\sum_j \tilde{A_{ij}},\tilde{A}=A+I_N</script><p>那么对于输入结点向量$X \in R^{N \times C}$，其中 N 为节点数，C 为节点的特征向量维度，我们有：</p><script type="math/tex; mode=display">Z=\tilde D^{-{\frac{1}{2}}}\tilde A\tilde D^{-{\frac{1}{2}}}X\Theta</script><p>$\Theta \in R^{C\times F}$是滤波器的参数矩阵，$Z\in R^{N \times F}$ 是卷积后的信号矩阵，时间复杂度$O(|E|FC)$</p><p>多层图卷积网络：</p><script type="math/tex; mode=display">H^{(l+1)}=\sigma(\tilde D^{-{\frac{1}{2}}}\tilde A\tilde D^{-{\frac{1}{2}}}H^{(l)}W^{(l)})</script><p>$W^{(l)}$为第$l$层权重矩阵，$\sigma(\cdot)$是激活函数，$H^{(l)}\in R^{N \times D}$是第$l$层激活函数，$H^{(0)}=X$</p><p>至于模型：</p><script type="math/tex; mode=display">Z=f(X,A)=softmax(\hat{A}ReLU(\hat AXW^{(0)})W^{(1)})</script><p>$W^{(0)}\in R^{C\times H}$从输入层到隐藏层，$W^{(1)}\in R^{H\times F}$从隐藏层到输出层</p><p>代价函数：</p><script type="math/tex; mode=display">L=-\sum_{l\in y_L}\sum_{f=1}^{F}Y_{lf}\ ln\ Z_{lf}</script><h3 id="2-4-GAT"><a href="#2-4-GAT" class="headerlink" title="2.4 GAT"></a>2.4 GAT</h3><p>GCN没有卷积、参数共享，还需要一个$\vec{a}$，是用一个结点收自己邻域信息时要共享的参数，是Self-Attention。</p><p>自己做完特征变换，邻居做完特征变换，然后组合一下，过一个Attention机制，得到一个权重。</p><script type="math/tex; mode=display">\alpha_{ij}=\frac{\exp(\mathrm{LeakyReLU}(\vec{a}^{T}[W\vec{h_i}||W\vec{h_j}]))}{\sum_{k\in \mathcal{N}_i}\exp(\mathrm{LeakyReLU}(\vec{a}^{T}[W\vec{h_i}||W\vec{h_j}]))}</script><p>$W$为特征变换参数，$\vec{a}$为注意力机制参数</p><h3 id="2-5-MoNet"><a href="#2-5-MoNet" class="headerlink" title="2.5 MoNet"></a>2.5 MoNet</h3><p>空间方法的一般框架。</p><p>定义各种各样的核函数，目的是为了度量目标结点与其他结点的相似度，卷积核则是这些核函数的权重。</p><script type="math/tex; mode=display">(f \ast g)=\sum_{j=1}^{J}g_jD_j(x)f</script><h2 id="3-Spectral-methods-vs-Spatial-methods"><a href="#3-Spectral-methods-vs-Spatial-methods" class="headerlink" title="3. Spectral methods vs. Spatial methods"></a>3. Spectral methods vs. Spatial methods</h2><h3 id="3-1-谱方法是空间方法的特例"><a href="#3-1-谱方法是空间方法的特例" class="headerlink" title="3.1 谱方法是空间方法的特例"></a>3.1 谱方法是空间方法的特例</h3><p>谱方法需要显式定义卷积核，而空间方法不需要。</p><script type="math/tex; mode=display">(f \ast g)=\sum_{j=1}^{J}g_jD_j(x)f</script><p>$D_j(x)$为核函数</p><h3 id="3-2-Spectral-methods"><a href="#3-2-Spectral-methods" class="headerlink" title="3.2 Spectral methods"></a>3.2 Spectral methods</h3><ul><li>Spectral CNN</li></ul><script type="math/tex; mode=display">y=Ug_\theta U^T x=(\theta_1 {\color{Red}{u_1u_1^T}}+\dots +\theta_n {\color{Red}{u_nu_n^T}})</script><ul><li>ChebyNet</li></ul><script type="math/tex; mode=display">y=(\theta_0{\color{Red}I}+\theta_1{\color{Red}L}+\dots+\theta_{K-1}{\color{Red}L^{K-1}})x</script><ul><li>GCN</li></ul><script type="math/tex; mode=display">y=\theta({\color{Red}I}-{\color{Red}L})x</script><h3 id="3-3-基本滤波器"><a href="#3-3-基本滤波器" class="headerlink" title="3.3 基本滤波器"></a>3.3 基本滤波器</h3><p>信号$\boldsymbol{x}$在图上的平滑程度被定义为</p><script type="math/tex; mode=display">x^TLx=\sum_{(u,v)\in E}A_{uv}\left(\frac{x_u}{\sqrt{d_u}}-\frac{x_v}{\sqrt{d_v}}\right)^2</script><p>特征值$\lambda_i=u_i^TLu_i$可以被视为特征向量$u_i$的频率。</p><p>每一个$u_iu_i^T$是一个基本的滤波器，只让频率$\lambda_i$的信号通过。</p><p>Spectral CNN恰是这些滤波器的线性组合。</p><p>ChebyNet的$L^k$是基础滤波器带有特殊系数$\left\{\lambda_i^k\right\}_{i=1}^n$的线性组合。</p><p>即认为信号频率越高，权重越大，实际上是对高频信号的加强，然而高频信号不体现节点分类的平滑性。</p><p>GCN之所以表现更好，是因为只考虑$k=0$和$k=1$，是对ChebyNet的低阶近似。</p><h3 id="3-4-GraphHeat"><a href="#3-4-GraphHeat" class="headerlink" title="3.4 GraphHeat"></a>3.4 GraphHeat</h3><p>设计低通滤波器</p><p>$\left\{e^{-skL}\right\}$，$e^{-sL}$是热核，通过图上热扩散定义相似度：</p><script type="math/tex; mode=display">e^{-sL}=Ue^{-s\Lambda}U^T,\Lambda=diag(\lambda_1,\dots,\lambda_n)</script><p>频率高的信号有一个$e$的指数衰减过程，$u_iu_i^T$的参数是$e^{-s\lambda_i}$</p><h2 id="4-Graph-Pooling"><a href="#4-Graph-Pooling" class="headerlink" title="4. Graph Pooling"></a>4. Graph Pooling</h2><p>早期图上任务关于结点级别，对应图像的像素级别，不需要pooling。</p><h3 id="4-1-Graph-coarsening"><a href="#4-1-Graph-coarsening" class="headerlink" title="4.1 Graph coarsening"></a>4.1 Graph coarsening</h3><p>类似下采样，对图做聚类，最后将每个聚类变成超级结点。</p><h3 id="4-2-Node-selection"><a href="#4-2-Node-selection" class="headerlink" title="4.2 Node selection"></a>4.2 Node selection</h3><p>每做一层厚，选一部分结点作为代表，需要量化结点重要性的度量。</p><h2 id="5-Questions"><a href="#5-Questions" class="headerlink" title="5. Questions"></a>5. Questions</h2><ul><li><strong>Does structure matters?</strong></li><li><strong>Context representation?</strong></li><li><strong>Future applications?</strong></li></ul><ol><li><a href="https://zhuanlan.zhihu.com/p/120311352">https://zhuanlan.zhihu.com/p/120311352</a></li><li><a href="https://zhuanlan.zhihu.com/p/290755442">https://zhuanlan.zhihu.com/p/290755442</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="posts/4278564287/"/>
      <url>posts/4278564287/</url>
      
        <content type="html"><![CDATA[<h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><p>代码测试：</p><pre class="line-numbers language-py" data-language="py"><code class="language-py">print("Hello")print("this is second.")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>引用测试：</p><blockquote><p>这是一条引用</p></blockquote><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><ul><li>哈哈</li><li>嘿嘿</li></ul><script type="math/tex; mode=display">lim_{1\to+\infty}P(|\frac{1}{n}\sum_i^nX_i-\mu|<\epsilon)=1, i=1,...,n</script><p><img src="https://cdn.jsdelivr.net/gh/zxwx1111/Figurebed/img/images.png" alt=""></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="posts/1243066710/"/>
      <url>posts/1243066710/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
